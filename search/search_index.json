{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hashi Learning Let's get our learning on with all things Hashicorp (and a little more). Background Some of us like to get things up and running extremely quick and not have to worry about the mundane tasks involved. This repo will do exactly that. By leveraging Vagrant and Ansible to do all of the heavy lifting. The functionality of this repo will grow over time iteratively. So, keep an eye out for new functionality over time.","title":"Hashi Learning"},{"location":"#hashi-learning","text":"Let's get our learning on with all things Hashicorp (and a little more).","title":"Hashi Learning"},{"location":"#background","text":"Some of us like to get things up and running extremely quick and not have to worry about the mundane tasks involved. This repo will do exactly that. By leveraging Vagrant and Ansible to do all of the heavy lifting. The functionality of this repo will grow over time iteratively. So, keep an eye out for new functionality over time.","title":"Background"},{"location":"01_Getting_Started/","text":"01. Getting Started Cloning Repo First let's go ahead and clone this repo. Because we use Git submodules, ensure that you clone exactly like below: git clone https://github.com/mrlesmithjr/hashi-learning.git --recursive Pre-Reqs NOTE: Windows support may be limited, but we are open to PR's, etc. Python3 VirtualBox Python3 NOTE: Ensure that you have a working Python3 setup as we will be using a Python virtualenv as part of this repo. Now let's setup our Python virtualenv. We will be using a virtualenv to ensure not to clutter up your existing Python environment. pip3 install virtualenv python3 -m venv venv source venv/bin/activate pip3 install --upgrade pip pip-tools pip-sync requirements.txt requirements-dev.txt VirtualBox We will be using VirtualBox for our Virtualization. Vagrant will also be using Virtualbox for it's provider throughout this learning. Follow the setup for Virtualbox here .","title":"01. Getting Started"},{"location":"01_Getting_Started/#01-getting-started","text":"","title":"01. Getting Started"},{"location":"01_Getting_Started/#cloning-repo","text":"First let's go ahead and clone this repo. Because we use Git submodules, ensure that you clone exactly like below: git clone https://github.com/mrlesmithjr/hashi-learning.git --recursive","title":"Cloning Repo"},{"location":"01_Getting_Started/#pre-reqs","text":"NOTE: Windows support may be limited, but we are open to PR's, etc. Python3 VirtualBox","title":"Pre-Reqs"},{"location":"01_Getting_Started/#python3","text":"NOTE: Ensure that you have a working Python3 setup as we will be using a Python virtualenv as part of this repo. Now let's setup our Python virtualenv. We will be using a virtualenv to ensure not to clutter up your existing Python environment. pip3 install virtualenv python3 -m venv venv source venv/bin/activate pip3 install --upgrade pip pip-tools pip-sync requirements.txt requirements-dev.txt","title":"Python3"},{"location":"01_Getting_Started/#virtualbox","text":"We will be using VirtualBox for our Virtualization. Vagrant will also be using Virtualbox for it's provider throughout this learning. Follow the setup for Virtualbox here .","title":"VirtualBox"},{"location":"02_Packer/","text":"02. Packer HashiCorp Packer automates the creation of any type of machine image. It embraces modern configuration management by encouraging you to use automated scripts to install and configure the software within your Packer-made images. Packer brings machine images into the modern age, unlocking untapped potential and opening new opportunities. Installing Before we can get started using Packer, we need to first install it. So, please follow the instructions found here based on your OS. Template Now that Packer is installed, let's begin walking through what is required to build our first image. The very first thing you'll need, is a template. A Packer template is the configuration that describes the image that you intend to build. This template is written in JSON. We have included a Ubuntu 18.04 template as part of this repository that will be used. You can find this template in learning/Packer , and the template name is ubuntu1804.json . NOTE: Our Ubuntu 18.04 image will be what we use from here on our throughout this learning. Ubuntu 18.04 Template The following template is the one we will be using here in a few moments. { \"builders\": [ { \"boot_command\": [ \"{{ user `boot_command_prefix` }}\", \"<wait>\", \"/install/vmlinuz\", \"<wait>\", \" initrd=/install/initrd.gz\", \"<wait>\", \" auto=true\", \"<wait>\", \" priority=critical\", \"<wait>\", \" url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ubuntu/preseed.cfg <wait>\", \"<wait>\", \"<enter>\" ], \"cpus\": \"{{ user `vm_vcpu` }}\", \"disk_size\": \"{{ user `vm_disk_size` }}\", \"guest_os_type\": \"Ubuntu_64\", \"hard_drive_interface\": \"{{ user `vm_disk_adapter_type` }}\", \"headless\": true, \"http_directory\": \"http\", \"iso_checksum\": \"{{ user `iso_checksum` }}\", \"iso_url\": \"{{ user `iso_url` }}\", \"memory\": \"{{ user `vm_memory` }}\", \"output_directory\": \"output-{{ user `vm_name` }}-{{ build_type }}-{{ timestamp }}\", \"shutdown_command\": \"echo '/sbin/halt -h -p' > shutdown.sh; echo 'packer'|sudo -S bash 'shutdown.sh'\", \"ssh_password\": \"{{ user `vm_ssh_password` }}\", \"ssh_timeout\": \"60m\", \"ssh_username\": \"{{ user `vm_ssh_username` }}\", \"type\": \"virtualbox-iso\", \"vm_name\": \"{{ user `vm_name` }}-{{ timestamp }}\" }, { \"boot_command\": [ \"{{ user `boot_command_prefix` }}\", \"<wait>\", \"/install/vmlinuz\", \"<wait>\", \" initrd=/install/initrd.gz\", \"<wait>\", \" auto=true\", \"<wait>\", \" priority=critical\", \"<wait>\", \" url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ubuntu/preseed.cfg <wait>\", \"<wait>\", \"<enter>\" ], \"cpus\": \"{{ user `vm_vcpu` }}\", \"disk_adapter_type\": \"{{ user `vm_disk_adapter_type` }}\", \"disk_size\": \"{{ user `vm_disk_size` }}\", \"guest_os_type\": \"ubuntu-64\", \"headless\": true, \"http_directory\": \"http\", \"iso_checksum\": \"{{ user `iso_checksum` }}\", \"iso_url\": \"{{ user `iso_url` }}\", \"memory\": \"{{ user `vm_memory` }}\", \"output_directory\": \"output-{{ user `vm_name` }}-{{ build_type }}-{{ timestamp }}\", \"shutdown_command\": \"echo '/sbin/halt -h -p' > shutdown.sh; echo 'packer'|sudo -S bash 'shutdown.sh'\", \"ssh_password\": \"{{ user `vm_ssh_password` }}\", \"ssh_timeout\": \"60m\", \"ssh_username\": \"{{ user `vm_ssh_username` }}\", \"type\": \"vmware-iso\", \"vm_name\": \"{{ user `vm_name` }}-{{ timestamp }}\" } ], \"post-processors\": [ [ { \"compression_level\": \"{{ user `compression_level` }}\", \"output\": \"{{ user `vm_name` }}-{{.Provider}}-{{ timestamp }}.box\", \"type\": \"vagrant\" }, { \"output\": \"manifest.json\", \"strip_path\": true, \"type\": \"manifest\" } ] ], \"provisioners\": [ { \"scripts\": [ \"scripts/base.sh\", \"scripts/vagrant.sh\", \"scripts/virtualbox.sh\", \"scripts/vmware.sh\", \"scripts/cleanup.sh\", \"scripts/zerodisk.sh\" ], \"type\": \"shell\" } ], \"variables\": { \"boot_command_prefix\": \"<enter><wait><f6><esc><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs>\", \"compression_level\": \"6\", \"iso_checksum\": \"e2ecdace33c939527cbc9e8d23576381c493b071107207d2040af72595f8990b\", \"iso_url\": \"http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso\", \"vm_disk_adapter_type\": \"scsi\", \"vm_disk_size\": \"36864\", \"vm_memory\": \"1024\", \"vm_name\": \"ubuntu1804\", \"vm_ssh_password\": \"vagrant\", \"vm_ssh_username\": \"vagrant\", \"vm_vcpu\": \"1\" } } User Variables User variables are defined within a template as variables . These variables allow us to parameterize portions of our templates and make them portable. We can also pass variables as arguments ( -var , -var-file ) when executing Packer to allow us to pass defaults along. By using the -var argument, we can define a specific variable such as: packer build -var username=superuser -var password=supersecretpassword template.json We can also use the -var-file argument to pass additional variables from a JSON file. An example of this could be: variables.json : { \"username\": \"superuser\", \"password\": \"supersecretpassword\" } And then we can pass this onto Packer: packer build -var-file variables.json template.json Builders Builders are what Packer uses to create and generate OS images. Builders are available for numerous different platforms and each one has their own specific configuration options. Some examples of Packer builders include: Azure DigitalOcean Vagrant VirtualBox VMware NOTE: Our ubuntu1804.json template has the following builders: virtualbox-iso and vmware-iso . Provisioners Provisioners are an array of provisioners that Packer will use to install, configure, etc. within our image. These may include Ansible, Chef, Puppet, Shell scripts, Powershell, etc. Provisioners are also optional within a template. Post-Processors Post-processors are tasks that execute after any provisioners that may be defined. Otherwise, after the builders complete. Post-processors are used for artifacts, conversions, etc. Vagrant The Vagrant post-processor comsumes an existing build and then converts it to a consumable Vagrant box. One thing to note is that all Vagrant boxes are provider specific. So, whichever Packer builder is used when creating an image. The Vagrant post-processor will generate the Vagrant box for that provider. Validating Now that we have the template that we will be using, we should first validate it. So, let's go ahead and do that now. cd learning/Packer packer validate ubuntu1804.json Building Now that our template has been validated, we are ready to build our first image. So, if you are not currently in the learning/Packer directory, let's go ahead and change into that directory. Now that we're in the learning/Packer directory, let's go ahead and build our image. Because we are using VirtualBox as our default virtualization, we will only build for our virtualbox-iso builder. packer build -only virtualbox-iso ubuntu1804.json \u25b6 packer build -only virtualbox-iso ubuntu1804.json virtualbox-iso: output will be in this color. ==> virtualbox-iso: Retrieving Guest additions ==> virtualbox-iso: Trying /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso ==> virtualbox-iso: Trying /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso ==> virtualbox-iso: /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso => /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso ==> virtualbox-iso: Retrieving ISO ==> virtualbox-iso: Trying http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso ==> virtualbox-iso: Trying http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso?checksum=sha256%3Ae2ecdace33c939527cbc9e8d23576381c493b071107207d2040af72595f8990b ==> virtualbox-iso: http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso?checksum=sha256%3Ae2ecdace33c939527cbc9e8d23576381c493b071107207d2040af72595f8990b => /Users/larrysmithjr/Git_Projects/Personal/GitHub/mrlesmithjr/hashi-learning/learning/Packer/packer_cache/23ac40ee61da5cc4ecaa017cd6d33a162365c3ac.iso ==> virtualbox-iso: Starting HTTP server on port 8046 ==> virtualbox-iso: Creating virtual machine... ==> virtualbox-iso: Creating hard drive... ==> virtualbox-iso: Creating forwarded port mapping for communicator (SSH, WinRM, etc) (host port 3952) ==> virtualbox-iso: Starting the virtual machine... virtualbox-iso: The VM will be run headless, without a GUI. If you want to virtualbox-iso: view the screen of the VM, connect via VRDP without a password to virtualbox-iso: rdp://127.0.0.1:5933 ==> virtualbox-iso: Waiting 10s for boot... ==> virtualbox-iso: Typing the boot command... ==> virtualbox-iso: Using ssh communicator to connect: 127.0.0.1 ==> virtualbox-iso: Waiting for SSH to become available... ... ==> Builds finished. The artifacts of successful builds are: --> virtualbox-iso: 'virtualbox' provider box: ubuntu1804-virtualbox-1595459323.box --> virtualbox-iso: And once the build completes you will have a ubuntu-virtualbox-**********.box file which is our Vagrant box image that we will be using. In the example above, my Vagrant box file was ubuntu1804-virtualbox-1595459323.box . Consuming We will explore importing this new generated image in 03_Vagrant and see how we can consume this image with Vagrant.","title":"02. Packer"},{"location":"02_Packer/#02-packer","text":"HashiCorp Packer automates the creation of any type of machine image. It embraces modern configuration management by encouraging you to use automated scripts to install and configure the software within your Packer-made images. Packer brings machine images into the modern age, unlocking untapped potential and opening new opportunities.","title":"02. Packer"},{"location":"02_Packer/#installing","text":"Before we can get started using Packer, we need to first install it. So, please follow the instructions found here based on your OS.","title":"Installing"},{"location":"02_Packer/#template","text":"Now that Packer is installed, let's begin walking through what is required to build our first image. The very first thing you'll need, is a template. A Packer template is the configuration that describes the image that you intend to build. This template is written in JSON. We have included a Ubuntu 18.04 template as part of this repository that will be used. You can find this template in learning/Packer , and the template name is ubuntu1804.json . NOTE: Our Ubuntu 18.04 image will be what we use from here on our throughout this learning.","title":"Template"},{"location":"02_Packer/#ubuntu-1804-template","text":"The following template is the one we will be using here in a few moments. { \"builders\": [ { \"boot_command\": [ \"{{ user `boot_command_prefix` }}\", \"<wait>\", \"/install/vmlinuz\", \"<wait>\", \" initrd=/install/initrd.gz\", \"<wait>\", \" auto=true\", \"<wait>\", \" priority=critical\", \"<wait>\", \" url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ubuntu/preseed.cfg <wait>\", \"<wait>\", \"<enter>\" ], \"cpus\": \"{{ user `vm_vcpu` }}\", \"disk_size\": \"{{ user `vm_disk_size` }}\", \"guest_os_type\": \"Ubuntu_64\", \"hard_drive_interface\": \"{{ user `vm_disk_adapter_type` }}\", \"headless\": true, \"http_directory\": \"http\", \"iso_checksum\": \"{{ user `iso_checksum` }}\", \"iso_url\": \"{{ user `iso_url` }}\", \"memory\": \"{{ user `vm_memory` }}\", \"output_directory\": \"output-{{ user `vm_name` }}-{{ build_type }}-{{ timestamp }}\", \"shutdown_command\": \"echo '/sbin/halt -h -p' > shutdown.sh; echo 'packer'|sudo -S bash 'shutdown.sh'\", \"ssh_password\": \"{{ user `vm_ssh_password` }}\", \"ssh_timeout\": \"60m\", \"ssh_username\": \"{{ user `vm_ssh_username` }}\", \"type\": \"virtualbox-iso\", \"vm_name\": \"{{ user `vm_name` }}-{{ timestamp }}\" }, { \"boot_command\": [ \"{{ user `boot_command_prefix` }}\", \"<wait>\", \"/install/vmlinuz\", \"<wait>\", \" initrd=/install/initrd.gz\", \"<wait>\", \" auto=true\", \"<wait>\", \" priority=critical\", \"<wait>\", \" url=http://{{ .HTTPIP }}:{{ .HTTPPort }}/ubuntu/preseed.cfg <wait>\", \"<wait>\", \"<enter>\" ], \"cpus\": \"{{ user `vm_vcpu` }}\", \"disk_adapter_type\": \"{{ user `vm_disk_adapter_type` }}\", \"disk_size\": \"{{ user `vm_disk_size` }}\", \"guest_os_type\": \"ubuntu-64\", \"headless\": true, \"http_directory\": \"http\", \"iso_checksum\": \"{{ user `iso_checksum` }}\", \"iso_url\": \"{{ user `iso_url` }}\", \"memory\": \"{{ user `vm_memory` }}\", \"output_directory\": \"output-{{ user `vm_name` }}-{{ build_type }}-{{ timestamp }}\", \"shutdown_command\": \"echo '/sbin/halt -h -p' > shutdown.sh; echo 'packer'|sudo -S bash 'shutdown.sh'\", \"ssh_password\": \"{{ user `vm_ssh_password` }}\", \"ssh_timeout\": \"60m\", \"ssh_username\": \"{{ user `vm_ssh_username` }}\", \"type\": \"vmware-iso\", \"vm_name\": \"{{ user `vm_name` }}-{{ timestamp }}\" } ], \"post-processors\": [ [ { \"compression_level\": \"{{ user `compression_level` }}\", \"output\": \"{{ user `vm_name` }}-{{.Provider}}-{{ timestamp }}.box\", \"type\": \"vagrant\" }, { \"output\": \"manifest.json\", \"strip_path\": true, \"type\": \"manifest\" } ] ], \"provisioners\": [ { \"scripts\": [ \"scripts/base.sh\", \"scripts/vagrant.sh\", \"scripts/virtualbox.sh\", \"scripts/vmware.sh\", \"scripts/cleanup.sh\", \"scripts/zerodisk.sh\" ], \"type\": \"shell\" } ], \"variables\": { \"boot_command_prefix\": \"<enter><wait><f6><esc><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs><bs>\", \"compression_level\": \"6\", \"iso_checksum\": \"e2ecdace33c939527cbc9e8d23576381c493b071107207d2040af72595f8990b\", \"iso_url\": \"http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso\", \"vm_disk_adapter_type\": \"scsi\", \"vm_disk_size\": \"36864\", \"vm_memory\": \"1024\", \"vm_name\": \"ubuntu1804\", \"vm_ssh_password\": \"vagrant\", \"vm_ssh_username\": \"vagrant\", \"vm_vcpu\": \"1\" } }","title":"Ubuntu 18.04 Template"},{"location":"02_Packer/#user-variables","text":"User variables are defined within a template as variables . These variables allow us to parameterize portions of our templates and make them portable. We can also pass variables as arguments ( -var , -var-file ) when executing Packer to allow us to pass defaults along. By using the -var argument, we can define a specific variable such as: packer build -var username=superuser -var password=supersecretpassword template.json We can also use the -var-file argument to pass additional variables from a JSON file. An example of this could be: variables.json : { \"username\": \"superuser\", \"password\": \"supersecretpassword\" } And then we can pass this onto Packer: packer build -var-file variables.json template.json","title":"User Variables"},{"location":"02_Packer/#builders","text":"Builders are what Packer uses to create and generate OS images. Builders are available for numerous different platforms and each one has their own specific configuration options. Some examples of Packer builders include: Azure DigitalOcean Vagrant VirtualBox VMware NOTE: Our ubuntu1804.json template has the following builders: virtualbox-iso and vmware-iso .","title":"Builders"},{"location":"02_Packer/#provisioners","text":"Provisioners are an array of provisioners that Packer will use to install, configure, etc. within our image. These may include Ansible, Chef, Puppet, Shell scripts, Powershell, etc. Provisioners are also optional within a template.","title":"Provisioners"},{"location":"02_Packer/#post-processors","text":"Post-processors are tasks that execute after any provisioners that may be defined. Otherwise, after the builders complete. Post-processors are used for artifacts, conversions, etc.","title":"Post-Processors"},{"location":"02_Packer/#vagrant","text":"The Vagrant post-processor comsumes an existing build and then converts it to a consumable Vagrant box. One thing to note is that all Vagrant boxes are provider specific. So, whichever Packer builder is used when creating an image. The Vagrant post-processor will generate the Vagrant box for that provider.","title":"Vagrant"},{"location":"02_Packer/#validating","text":"Now that we have the template that we will be using, we should first validate it. So, let's go ahead and do that now. cd learning/Packer packer validate ubuntu1804.json","title":"Validating"},{"location":"02_Packer/#building","text":"Now that our template has been validated, we are ready to build our first image. So, if you are not currently in the learning/Packer directory, let's go ahead and change into that directory. Now that we're in the learning/Packer directory, let's go ahead and build our image. Because we are using VirtualBox as our default virtualization, we will only build for our virtualbox-iso builder. packer build -only virtualbox-iso ubuntu1804.json \u25b6 packer build -only virtualbox-iso ubuntu1804.json virtualbox-iso: output will be in this color. ==> virtualbox-iso: Retrieving Guest additions ==> virtualbox-iso: Trying /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso ==> virtualbox-iso: Trying /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso ==> virtualbox-iso: /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso => /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso ==> virtualbox-iso: Retrieving ISO ==> virtualbox-iso: Trying http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso ==> virtualbox-iso: Trying http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso?checksum=sha256%3Ae2ecdace33c939527cbc9e8d23576381c493b071107207d2040af72595f8990b ==> virtualbox-iso: http://cdimage.ubuntu.com/releases/18.04/release/ubuntu-18.04.4-server-amd64.iso?checksum=sha256%3Ae2ecdace33c939527cbc9e8d23576381c493b071107207d2040af72595f8990b => /Users/larrysmithjr/Git_Projects/Personal/GitHub/mrlesmithjr/hashi-learning/learning/Packer/packer_cache/23ac40ee61da5cc4ecaa017cd6d33a162365c3ac.iso ==> virtualbox-iso: Starting HTTP server on port 8046 ==> virtualbox-iso: Creating virtual machine... ==> virtualbox-iso: Creating hard drive... ==> virtualbox-iso: Creating forwarded port mapping for communicator (SSH, WinRM, etc) (host port 3952) ==> virtualbox-iso: Starting the virtual machine... virtualbox-iso: The VM will be run headless, without a GUI. If you want to virtualbox-iso: view the screen of the VM, connect via VRDP without a password to virtualbox-iso: rdp://127.0.0.1:5933 ==> virtualbox-iso: Waiting 10s for boot... ==> virtualbox-iso: Typing the boot command... ==> virtualbox-iso: Using ssh communicator to connect: 127.0.0.1 ==> virtualbox-iso: Waiting for SSH to become available... ... ==> Builds finished. The artifacts of successful builds are: --> virtualbox-iso: 'virtualbox' provider box: ubuntu1804-virtualbox-1595459323.box --> virtualbox-iso: And once the build completes you will have a ubuntu-virtualbox-**********.box file which is our Vagrant box image that we will be using. In the example above, my Vagrant box file was ubuntu1804-virtualbox-1595459323.box .","title":"Building"},{"location":"02_Packer/#consuming","text":"We will explore importing this new generated image in 03_Vagrant and see how we can consume this image with Vagrant.","title":"Consuming"},{"location":"03_Vagrant/","text":"03. Vagrant Vagrant is a tool for building and managing virtual machine environments in a single workflow. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases production parity, and makes the \"works on my machine\" excuse a relic of the past. Boxes Adding For our learning, we will be adding the box we built using Packer back in 02. Packer . So, let's go ahead and do this now. NOTE: Replace the 1595459323 with whatever matches your build. vagrant box add local/ubuntu1804 ubuntu1804-virtualbox-1595459323.box ==> box: Box file was not detected as metadata. Adding it directly... ==> box: Adding box 'local/ubuntu1804' (v0) for provider: box: Unpacking necessary files from: file:///Users/larrysmithjr/Git_Projects/Personal/GitHub/mrlesmithjr/hashi-learning/learning/Packer/ubuntu1804-virtualbox-1595459323.box ==> box: Successfully added box 'local/ubuntu1804' (v0) for 'virtualbox'! We've successfully added our first Vagrant box! And as we mentioned previously, we will be using this image for the remainder of this learning. However, before we move on, let's add another box. vagrant box add mrlesmithjr/centos7 \u25b6 vagrant box add mrlesmithjr/centos7 ==> box: Loading metadata for box 'mrlesmithjr/centos7' box: URL: https://vagrantcloud.com/mrlesmithjr/centos7 ==> box: Adding box 'mrlesmithjr/centos7' (v1588339341) for provider: libvirt box: Downloading: https://vagrantcloud.com/mrlesmithjr/boxes/centos7/versions/1588339341/providers/libvirt.box Download redirected to host: vagrantcloud-files-production.s3.amazonaws.com ==> box: Successfully added box 'mrlesmithjr/centos7' (v1588339341) for 'libvirt'! We've now just successfully added a new box from Vagrant Cloud for CentOS 7 using the libvirt provider. Listing Now that we've added our first box, we can get a quick list of boxes currently available by executing: vagrant box list \u25b6 vagrant box list local/ubuntu1804 (virtualbox, 0) mrlesmithjr/centos7 (libvirt, 1588339341)","title":"03. Vagrant"},{"location":"03_Vagrant/#03-vagrant","text":"Vagrant is a tool for building and managing virtual machine environments in a single workflow. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases production parity, and makes the \"works on my machine\" excuse a relic of the past.","title":"03. Vagrant"},{"location":"03_Vagrant/#boxes","text":"","title":"Boxes"},{"location":"03_Vagrant/#adding","text":"For our learning, we will be adding the box we built using Packer back in 02. Packer . So, let's go ahead and do this now. NOTE: Replace the 1595459323 with whatever matches your build. vagrant box add local/ubuntu1804 ubuntu1804-virtualbox-1595459323.box ==> box: Box file was not detected as metadata. Adding it directly... ==> box: Adding box 'local/ubuntu1804' (v0) for provider: box: Unpacking necessary files from: file:///Users/larrysmithjr/Git_Projects/Personal/GitHub/mrlesmithjr/hashi-learning/learning/Packer/ubuntu1804-virtualbox-1595459323.box ==> box: Successfully added box 'local/ubuntu1804' (v0) for 'virtualbox'! We've successfully added our first Vagrant box! And as we mentioned previously, we will be using this image for the remainder of this learning. However, before we move on, let's add another box. vagrant box add mrlesmithjr/centos7 \u25b6 vagrant box add mrlesmithjr/centos7 ==> box: Loading metadata for box 'mrlesmithjr/centos7' box: URL: https://vagrantcloud.com/mrlesmithjr/centos7 ==> box: Adding box 'mrlesmithjr/centos7' (v1588339341) for provider: libvirt box: Downloading: https://vagrantcloud.com/mrlesmithjr/boxes/centos7/versions/1588339341/providers/libvirt.box Download redirected to host: vagrantcloud-files-production.s3.amazonaws.com ==> box: Successfully added box 'mrlesmithjr/centos7' (v1588339341) for 'libvirt'! We've now just successfully added a new box from Vagrant Cloud for CentOS 7 using the libvirt provider.","title":"Adding"},{"location":"03_Vagrant/#listing","text":"Now that we've added our first box, we can get a quick list of boxes currently available by executing: vagrant box list \u25b6 vagrant box list local/ubuntu1804 (virtualbox, 0) mrlesmithjr/centos7 (libvirt, 1588339341)","title":"Listing"},{"location":"04_Scenarios/","text":"04. Scenarios NOTE: All scenarios will have at a minimum of a single Consul server. In order to facilitate various scenarios and keep resources to a minimum, we have added scenarios for learning. So, to spin up a desired scenario, choose one from below and follow any of the instructions required. Consul - A single node Consul server Consul Cluster - A three node Consul cluster Consul Service Discovery with HAProxy Consul NGINX Reverse Proxy Vault - A single node Vault server Vault HA - Vault high availability w/Consul cluster storage","title":"04. Scenarios"},{"location":"04_Scenarios/#04-scenarios","text":"NOTE: All scenarios will have at a minimum of a single Consul server. In order to facilitate various scenarios and keep resources to a minimum, we have added scenarios for learning. So, to spin up a desired scenario, choose one from below and follow any of the instructions required. Consul - A single node Consul server Consul Cluster - A three node Consul cluster Consul Service Discovery with HAProxy Consul NGINX Reverse Proxy Vault - A single node Vault server Vault HA - Vault high availability w/Consul cluster storage","title":"04. Scenarios"},{"location":"05_Consul/","text":"05. Consul - Single Node In this scenario, we will simply spin up a single node Consul server which will provide us a learning environment to get familiar with basic Consul concepts. Spinning Up First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/consul.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'consul01' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* consul01 : ok=30 changed=13 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 And once all of the provisioning finishes, we will have a single node Consul server ready for us to explore. Nodes This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11 CLI Now we are ready to begin exploring our Consul server using the CLI. So, to do this. Let's SSH into our consul01 node. vagrant ssh consul01 \u25b6 vagrant ssh consul01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage Last login: Mon Jul 20 14:45:45 2020 from 10.0.2.2 vagrant@consul01:~$ Now that we've logged in, let's change into the /etc/consul.d directory and see what files/directories are present. cd /etc/consul.d ls -la vagrant@consul01:~$ cd /etc/consul.d vagrant@consul01:/etc/consul.d$ ls -la total 20 drwxr-xr-x 4 root root 4096 Jul 20 14:36 . drwxr-xr-x 79 root root 4096 Jul 20 14:36 .. drwxr-xr-x 2 root root 4096 Jul 20 14:36 client -rw-r--r-- 1 root root 846 Jul 20 14:36 config.json drwxr-xr-x 2 root root 4096 Jul 20 14:36 scripts vagrant@consul01:/etc/consul.d$ CLI - Configuration Let's first take a quick look at our Consul configuration while we're here. cat config.json vagrant@consul01:/etc/consul.d$ cat config.json { \"acl\": { \"default_policy\": \"allow\", \"down_policy\": \"extend-cache\", \"tokens\": { \"agent\": \"\", \"agent_master\": \"\", \"default\": \"\", \"master\": \"6DA12E0F-D8A5-48C5-AEFF-00D50E84D01A\", \"replication\": \"\" } }, \"bind_addr\": \"192.168.250.11\", \"bootstrap_expect\": 1, \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul\", \"datacenter\": \"dc1\", \"dns_config\": {}, \"enable_acl_replication\": false, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"consul01\", \"performance\": {}, \"primary_datacenter\": \"dc1\", \"retry_join\": [ \"192.168.250.11\" ], \"retry_join_wan\": [], \"server\": true, \"telemetry\": {}, \"ui\": true } vagrant@consul01:/etc/consul.d$ And if we wanted to do a quick validation of our Consul configuration, we can do so by: consul validate /etc/consul.d vagrant@consul01:/etc/consul.d$ consul validate /etc/consul.d Configuration is valid! vagrant@consul01:/etc/consul.d$ CLI - Datacenters Next, let's see what Consul datacenters exist: consul catalog datacenters vagrant@consul01:/etc/consul.d$ consul catalog datacenters dc1 vagrant@consul01:/etc/consul.d$ We can also use curl to retrieve the same information: curl --silent http://127.0.0.1:8500/v1/catalog/datacenters | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/datacenters | jq [ \"dc1\" ] vagrant@consul01:/etc/consul.d$ CLI - Services Next, let's check and see what services are available: consul catalog services vagrant@consul01:/etc/consul.d$ consul catalog services consul vagrant@consul01:/etc/consul.d$ We can also use curl to retrieve the same information: curl --silent http://127.0.0.1:8500/v1/catalog/services | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/services | jq { \"consul\": [] } vagrant@consul01:/etc/consul.d$ CLI - Nodes Next, let's get a list of nodes available: consul catalog nodes vagrant@consul01:/etc/consul.d$ consul catalog nodes Node ID Address DC consul01 290c5be7 192.168.250.11 dc1 vagrant@consul01:/etc/consul.d$ The command above gives us a simplistic view of our nodes, but if we'd like to get a more detailed view we can execute the following: consul catalog nodes --detailed vagrant@consul01:/etc/consul.d$ consul catalog nodes --detailed Node ID Address DC TaggedAddresses Meta consul01 290c5be7-7a03-7e9a-5db4-d87f912c1674 192.168.250.11 dc1 lan=192.168.250.11, lan_ipv4=192.168.250.11, wan=192.168.250.11, wan_ipv4=192.168.250.11 consul-network-segment= vagrant@consul01:/etc/consul.d$ As you can see, the above command gives us a much better detailed view into our nodes. We can also use curl to get the same information: curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq [ { \"ID\": \"290c5be7-7a03-7e9a-5db4-d87f912c1674\", \"Node\": \"consul01\", \"Address\": \"192.168.250.11\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.11\", \"lan_ipv4\": \"192.168.250.11\", \"wan\": \"192.168.250.11\", \"wan_ipv4\": \"192.168.250.11\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 5, \"ModifyIndex\": 6 } ] vagrant@consul01:/etc/consul.d$ And if we'd like to get detailed information for node consul01 using curl : curl --silent http://127.0.0.1:8500/v1/catalog/node/consul01 | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/node/consul01 | jq { \"Node\": { \"ID\": \"290c5be7-7a03-7e9a-5db4-d87f912c1674\", \"Node\": \"consul01\", \"Address\": \"192.168.250.11\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.11\", \"lan_ipv4\": \"192.168.250.11\", \"wan\": \"192.168.250.11\", \"wan_ipv4\": \"192.168.250.11\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 5, \"ModifyIndex\": 6 }, \"Services\": { \"consul\": { \"ID\": \"consul\", \"Service\": \"consul\", \"Tags\": [], \"Address\": \"\", \"Meta\": { \"raft_version\": \"3\", \"serf_protocol_current\": \"2\", \"serf_protocol_max\": \"5\", \"serf_protocol_min\": \"1\", \"version\": \"1.7.2\" }, \"Port\": 8300, \"Weights\": { \"Passing\": 1, \"Warning\": 1 }, \"EnableTagOverride\": false, \"Proxy\": { \"MeshGateway\": {}, \"Expose\": {} }, \"Connect\": {}, \"CreateIndex\": 5, \"ModifyIndex\": 5 } } } vagrant@consul01:/etc/consul.d$ We can also get the health of our consul01 node using curl : curl --silent http://127.0.0.1:8500/v1/health/node/consul01 | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/health/node/consul01 | jq [ { \"Node\": \"consul01\", \"CheckID\": \"serfHealth\", \"Name\": \"Serf Health Status\", \"Status\": \"passing\", \"Notes\": \"\", \"Output\": \"Agent alive and reachable\", \"ServiceID\": \"\", \"ServiceName\": \"\", \"ServiceTags\": [], \"Type\": \"\", \"Definition\": {}, \"CreateIndex\": 5, \"ModifyIndex\": 5 } ] vagrant@consul01:/etc/consul.d$ CLI - Datacenter Members To get a list of Consul datacenter members, we can simply execute: consul members list vagrant@consul01:/etc/consul.d$ consul members list Node Address Status Type Build Protocol DC Segment consul01 192.168.250.11:8301 alive server 1.7.2 2 dc1 <all> vagrant@consul01:/etc/consul.d$ Next let's take a look at consul01 information specifically. consul info vagrant@consul01:/etc/consul.d$ consul info agent: check_monitors = 0 check_ttls = 0 checks = 0 services = 0 build: prerelease = revision = 9ea1a204 version = 1.7.2 consul: acl = disabled bootstrap = true known_datacenters = 1 leader = true leader_addr = 192.168.250.11:8300 server = true raft: applied_index = 142 commit_index = 142 fsm_pending = 0 last_contact = 0 last_log_index = 142 last_log_term = 3 last_snapshot_index = 0 last_snapshot_term = 0 latest_configuration = [{Suffrage:Voter ID:290c5be7-7a03-7e9a-5db4-d87f912c1674 Address:192.168.250.11:8300}] latest_configuration_index = 0 num_peers = 0 protocol_version = 3 protocol_version_max = 3 protocol_version_min = 0 snapshot_version_max = 1 snapshot_version_min = 0 state = Leader term = 3 runtime: arch = amd64 cpu_count = 1 goroutines = 81 max_procs = 2 os = linux version = go1.13.7 serf_lan: coordinate_resets = 0 encrypted = true event_queue = 1 event_time = 3 failed = 0 health_score = 0 intent_queue = 1 left = 0 member_time = 3 members = 1 query_queue = 0 query_time = 1 serf_wan: coordinate_resets = 0 encrypted = true event_queue = 0 event_time = 1 failed = 0 health_score = 0 intent_queue = 0 left = 0 member_time = 1 members = 1 query_queue = 0 query_time = 1 vagrant@consul01:/etc/consul.d$ UI Now that we've explored our Consul setup using some basic CLI commands, let's jump into the Consul UI. Open your preferred browser of choice and head over to http://192.168.250.11:8500/ui UI - Services Click on services and you'll see we only have one service currently in Consul, which will look like below. And if you click on the Consul service to dig in further, you'll see a bit of information on our Consul service. UI - Nodes Next, let's click on nodes and we SHOULD only see our consul01 node listed. Let's now click on consul01 and dig in a bit more on this node. DNS One of the beautiful things with Consul is that we have access to a DNS interface. The DNS interfaces provides us with a way to perform various lookups using normal DNS methods. This removes the need to use the API to do a lookup of any kind. One thing to note here is: The Consul DNS interface listens on port 8600 by default. NOTE: We will explore options to get the DNS interface to become transparent to systems in a later excercise. DNS - Node Lookups One example of using the DNS interface is to lookup nodes and get a list of records available. Let's see what DNS returns for our consul01 node. dig @127.0.0.1 -p 8600 consul01.node.consul ANY vagrant@consul01:/etc/consul.d$ dig @127.0.0.1 -p 8600 consul01.node.consul ANY ; <<>> DiG 9.11.3-1ubuntu1.11-Ubuntu <<>> @127.0.0.1 -p 8600 consul01.node.consul ANY ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 53553 ;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;consul01.node.consul. IN ANY ;; ANSWER SECTION: consul01.node.consul. 0 IN A 192.168.250.11 consul01.node.consul. 0 IN TXT \"consul-network-segment=\" ;; Query time: 1 msec ;; SERVER: 127.0.0.1#8600(127.0.0.1) ;; WHEN: Mon Jul 20 19:50:35 UTC 2020 ;; MSG SIZE rcvd: 101 vagrant@consul01:/etc/consul.d$ DNS - Service Lookups We can also use Consul to perform service lookups. The context of a service lookup looks like: [tag.]<service>.service[.datacenter].<domain> . The tag is an optional field that can be used to find a specific tag when services may be named the same but are used for entirely different things. For example, we may have multiple db services registered. But we could use tags to delineate between development and production . So, any application for development could find it's db service by using development.db.service.consul . Also, the datacenter is optional unless you need to find a service outside of your Consul agent's default Consul datacenter. So, if you are looking for a service in the same Consul datacenter, you can skip the datacenter portion. Otherwise, you can add the datacenter to find a service in another Consul datacenter. Seeing as we don't have any services registered at this point, we will just use the default consul service to perform an example DNS service lookup. dig @127.0.0.1 -p 8600 consul.service.consul vagrant@consul01:/etc/consul.d$ dig @127.0.0.1 -p 8600 consul.service.consul ; <<>> DiG 9.11.3-1ubuntu1.11-Ubuntu <<>> @127.0.0.1 -p 8600 consul.service.consul ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 5946 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;consul.service.consul. IN A ;; ANSWER SECTION: consul.service.consul. 0 IN A 192.168.250.11 ;; Query time: 0 msec ;; SERVER: 127.0.0.1#8600(127.0.0.1) ;; WHEN: Mon Jul 20 20:06:08 UTC 2020 ;; MSG SIZE rcvd: 66 vagrant@consul01:/etc/consul.d$ DNS - systemd-resolved Oh my! systemd-resolved What a pain. However, we can work around this relatively easy and not disable it. Many times I have opted to just disable systemd-resolved and install Dnsmasq and move on. But for this learnining we will install Dnsmasq , reconfigure systemd-resolved to forward to Dnsmasq , and configure Dnsmasq to forward the consul domain to our Consul client's DNS port. We will be doing this on all of our servers within our stack to ensure they are all consistent. As well as, properly resolve services registered in Consul without the need to do specific work arounds for various things such as: NGINX, etc. The benefit will be to understand how we can leverge Consul in any environment. To easily reach our services without load balancers, etc. For example, we might have an NGINX web server configured as a reverse proxy for many different sites within our environment. Think of our NGINX reverse proxy functioning as a form of an application gateway. We could frontend our application gateways with an external load balancer, which sends all HTTP(s) traffic to our application gateways. Then our application gateways can make decisions based on the URL, to properly redirect to our applications. The point here is that we could easily just define resolvers for NGINX, but that wouldn't solve OS level DNS resolution for services such as logging, etc. So, what does this look like after we provision our servers? Let's jump in and take a look. vagrant ssh consul01 Next, let's look at our systemd-resolved configuration and then Dnsmasq. cat /etc/systemd/resolved.conf vagrant@consul01:~$ cat /etc/systemd/resolved.conf # This file is part of systemd. # # systemd is free software; you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. # # Entries in this file show the compile time defaults. # You can change settings by editing this file. # Defaults can be restored by simply deleting this file. # # See resolved.conf(5) for details [Resolve] DNS=127.0.0.2 #FallbackDNS= #Domains= #LLMNR=no #MulticastDNS=no #DNSSEC=no #Cache=yes #DNSStubListener=yes vagrant@consul01:~$ The important thing from above is the line DNS=127.0.0.2 . This is telling systemd-resolved to forward all lookups to 127.0.0.2 which is what IP Dnsmasq is bound to. And if we take a quick look at /etc/dnsmasq.conf we can verify that is correct as well. cat /etc/dnsmasq.conf vagrant@consul01:~$ cat /etc/dnsmasq.conf bind-interfaces port=53 listen-address=127.0.0.2 server=/consul/127.0.0.1#8600 # Uncomment and modify as appropriate to enable reverse DNS lookups for # common netblocks found in RFC 1918, 5735, and 6598: #rev-server=0.0.0.0/8,127.0.0.1#8600 #rev-server=10.0.0.0/8,127.0.0.1#8600 #rev-server=100.64.0.0/10,127.0.0.1#8600 #rev-server=127.0.0.1/8,127.0.0.1#8600 #rev-server=169.254.0.0/16,127.0.0.1#8600 #rev-server=172.16.0.0/12,127.0.0.1#8600 #rev-server=192.168.0.0/16,127.0.0.1#8600 #rev-server=224.0.0.0/4,127.0.0.1#8600 #rev-server=240.0.0.0/4,127.0.0.1#8600 vagrant@consul01:~$ And as you can see, we have configured listen-address=127.0.0.2 and bind-interfaces to ensure that Dnsmasq only listens on that IP. We have also added server=/consul/127.0.0.1#8600 which tells Dnsmasq to forward all consul domains to thee local Consul client's DNS port(8600). Now let's verify that our DNS resolution is working properly to find registered services in Consul. dig consul.service.dc1.consul vagrant@consul01:~$ dig consul.service.dc1.consul ; <<>> DiG 9.11.3-1ubuntu1.11-Ubuntu <<>> consul.service.dc1.consul ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60558 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;consul.service.dc1.consul. IN A ;; ANSWER SECTION: consul.service.dc1.consul. 0 IN A 192.168.250.11 ;; Query time: 3 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Tue Jul 21 17:02:35 UTC 2020 ;; MSG SIZE rcvd: 70 vagrant@consul01:~$ KV Store The Consul KV store is another core feature of Consul. The KV store allows users to store indexed objects for various usages. The KV store is also a simple one that is not intended to replace a full featured KV datastore. The KV store does have an object size limitation of 512KB . Otherwise there are not any restrictions to its usage. KV Store - Examples To get familiar using the KV store, we will walk through some basic examples. KV Store - Examples - Put First, let's create a few KV pairs: consul kv put users/example/username example vagrant@consul01:/etc/consul.d$ consul kv put users/example/username example Success! Data written to: users/example/username vagrant@consul01:/etc/consul.d$ consul kv put users/example/email example@example.org vagrant@consul01:/etc/consul.d$ consul kv put users/example/email example@example.org Success! Data written to: users/example/email vagrant@consul01:/etc/consul.d$ consul kv put users/example/lists [\"list1\",\"list2\"] vagrant@consul01:/etc/consul.d$ consul kv put users/example/lists [\"list1\",\"list2\"] Success! Data written to: users/example/lists vagrant@consul01:/etc/consul.d$ You can also add the contents of a file as the value for a key. So, let's use an example YAML file that is stored in our project. consul kv put cloud/config @/vagrant/learning/Consul/kv_example.yaml vagrant@consul01:/etc/consul.d$ consul kv put cloud/config @/vagrant/learning/Consul/kv_example.yaml Success! Data written to: cloud/config vagrant@consul01:/etc/consul.d$ KV Store - Examples - Get Now let's explore how we can get the KV pairs. consul kv get users/example/email vagrant@consul01:/etc/consul.d$ consul kv get users/example/email example@example.org vagrant@consul01:/etc/consul.d$ consul kv get users/example/lists vagrant@consul01:/etc/consul.d$ consul kv get users/example/lists [list1,list2] vagrant@consul01:/etc/consul.d$ We can also get all of the keys defined within a path recursively. consul kv get --recurse users vagrant@consul01:/etc/consul.d$ consul kv get --recurse users users/example/email:example@example.org users/example/lists:[list1,list2] users/example/username:example vagrant@consul01:/etc/consul.d$ Now let's get the contents of the file that we added. consul kv get cloud/config vagrant@consul01:/etc/consul.d$ consul kv get cloud/config # ================================================================= # Cloud Information # ================================================================= cloud: # Cloud Name: The cloud name must not contain spaces or special # characters. The name is used for the OpenStack region name. name: MyCloudName # Cloud Description description: Controller + N Compute Topology - x86 KVM # Cloud Administrator (admin) User's Password password: MyCloudPassword # Cloud Database Service Type: db2 or mysql database_service_type: db2 # Cloud Messaging Service Type: rabbitmq or qpid messaging_service_type: rabbitmq # Cloud Features: The cloud features to be enabled or disabled. features: self_service_portal: enabled platform_resource_scheduler: enabled # Cloud Topology: References the node name(s) for each role # within the cloud's topology. topology: database_node_name: controller controller_node_name: controller self_service_portal_node_name: controller kvm_compute_node_names: kvm_compute # ================================================================ # Environment Information # ================================================================ environment: base: example-ibm-os-single-controller-n-compute default_attributes: # (Optional) Add Default Environment Attributes override_attributes: # (Optional) Add Override Environment Attributes # ================================================================ # Node Information # ================================================================ nodes: - name: controller description: Cloud controller node fqdn: controllername.company.com password: passw0rd identity_file: ~ nics: management_network: eth0 data_network: eth1 - name: kvm_compute description: Cloud KVM compute node fqdn: kvmcomputename.company.com password: ~ identity_file: /root/identity.pem nics: management_network: eth0 data_network: eth1 # (Optional) Node Attribute JSON File attribute_file: ~ vagrant@consul01:/etc/consul.d$ KV Store - Examples - Delete Now that we've explored adding and getting KV pairs from Consul. Let's now explore deleting those KV pairs. consul kv delete users/example/lists vagrant@consul01:/etc/consul.d$ consul kv delete users/example/lists Success! Deleted key: users/example/lists vagrant@consul01:/etc/consul.d$ consul kv delete cloud/config vagrant@consul01:/etc/consul.d$ consul kv delete cloud/config Success! Deleted key: cloud/config vagrant@consul01:/etc/consul.d$ And as we saw previously with recursively getting keys, we can also delete keys recursively. consul kv delete --recurse users vagrant@consul01:/etc/consul.d$ consul kv delete --recurse users Success! Deleted keys with prefix: users vagrant@consul01:/etc/consul.d$ Tearing Down After you've explored a single Consul node setup. You'll likely be ready to move onto more advanced scenarios. So, just as we did when spinning up. We can quickly tear everything down. ./scripts/cleanup.sh \u25b6 ./scripts/cleanup.sh ==> consul01: Forcing shutdown of VM... ==> consul01: Destroying VM and associated drives... (venv)","title":"05. Consul - Single Node"},{"location":"05_Consul/#05-consul-single-node","text":"In this scenario, we will simply spin up a single node Consul server which will provide us a learning environment to get familiar with basic Consul concepts.","title":"05. Consul - Single Node"},{"location":"05_Consul/#spinning-up","text":"First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/consul.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'consul01' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* consul01 : ok=30 changed=13 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 And once all of the provisioning finishes, we will have a single node Consul server ready for us to explore.","title":"Spinning Up"},{"location":"05_Consul/#nodes","text":"This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11","title":"Nodes"},{"location":"05_Consul/#cli","text":"Now we are ready to begin exploring our Consul server using the CLI. So, to do this. Let's SSH into our consul01 node. vagrant ssh consul01 \u25b6 vagrant ssh consul01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage Last login: Mon Jul 20 14:45:45 2020 from 10.0.2.2 vagrant@consul01:~$ Now that we've logged in, let's change into the /etc/consul.d directory and see what files/directories are present. cd /etc/consul.d ls -la vagrant@consul01:~$ cd /etc/consul.d vagrant@consul01:/etc/consul.d$ ls -la total 20 drwxr-xr-x 4 root root 4096 Jul 20 14:36 . drwxr-xr-x 79 root root 4096 Jul 20 14:36 .. drwxr-xr-x 2 root root 4096 Jul 20 14:36 client -rw-r--r-- 1 root root 846 Jul 20 14:36 config.json drwxr-xr-x 2 root root 4096 Jul 20 14:36 scripts vagrant@consul01:/etc/consul.d$","title":"CLI"},{"location":"05_Consul/#cli-configuration","text":"Let's first take a quick look at our Consul configuration while we're here. cat config.json vagrant@consul01:/etc/consul.d$ cat config.json { \"acl\": { \"default_policy\": \"allow\", \"down_policy\": \"extend-cache\", \"tokens\": { \"agent\": \"\", \"agent_master\": \"\", \"default\": \"\", \"master\": \"6DA12E0F-D8A5-48C5-AEFF-00D50E84D01A\", \"replication\": \"\" } }, \"bind_addr\": \"192.168.250.11\", \"bootstrap_expect\": 1, \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul\", \"datacenter\": \"dc1\", \"dns_config\": {}, \"enable_acl_replication\": false, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"consul01\", \"performance\": {}, \"primary_datacenter\": \"dc1\", \"retry_join\": [ \"192.168.250.11\" ], \"retry_join_wan\": [], \"server\": true, \"telemetry\": {}, \"ui\": true } vagrant@consul01:/etc/consul.d$ And if we wanted to do a quick validation of our Consul configuration, we can do so by: consul validate /etc/consul.d vagrant@consul01:/etc/consul.d$ consul validate /etc/consul.d Configuration is valid! vagrant@consul01:/etc/consul.d$","title":"CLI - Configuration"},{"location":"05_Consul/#cli-datacenters","text":"Next, let's see what Consul datacenters exist: consul catalog datacenters vagrant@consul01:/etc/consul.d$ consul catalog datacenters dc1 vagrant@consul01:/etc/consul.d$ We can also use curl to retrieve the same information: curl --silent http://127.0.0.1:8500/v1/catalog/datacenters | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/datacenters | jq [ \"dc1\" ] vagrant@consul01:/etc/consul.d$","title":"CLI - Datacenters"},{"location":"05_Consul/#cli-services","text":"Next, let's check and see what services are available: consul catalog services vagrant@consul01:/etc/consul.d$ consul catalog services consul vagrant@consul01:/etc/consul.d$ We can also use curl to retrieve the same information: curl --silent http://127.0.0.1:8500/v1/catalog/services | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/services | jq { \"consul\": [] } vagrant@consul01:/etc/consul.d$","title":"CLI - Services"},{"location":"05_Consul/#cli-nodes","text":"Next, let's get a list of nodes available: consul catalog nodes vagrant@consul01:/etc/consul.d$ consul catalog nodes Node ID Address DC consul01 290c5be7 192.168.250.11 dc1 vagrant@consul01:/etc/consul.d$ The command above gives us a simplistic view of our nodes, but if we'd like to get a more detailed view we can execute the following: consul catalog nodes --detailed vagrant@consul01:/etc/consul.d$ consul catalog nodes --detailed Node ID Address DC TaggedAddresses Meta consul01 290c5be7-7a03-7e9a-5db4-d87f912c1674 192.168.250.11 dc1 lan=192.168.250.11, lan_ipv4=192.168.250.11, wan=192.168.250.11, wan_ipv4=192.168.250.11 consul-network-segment= vagrant@consul01:/etc/consul.d$ As you can see, the above command gives us a much better detailed view into our nodes. We can also use curl to get the same information: curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq [ { \"ID\": \"290c5be7-7a03-7e9a-5db4-d87f912c1674\", \"Node\": \"consul01\", \"Address\": \"192.168.250.11\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.11\", \"lan_ipv4\": \"192.168.250.11\", \"wan\": \"192.168.250.11\", \"wan_ipv4\": \"192.168.250.11\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 5, \"ModifyIndex\": 6 } ] vagrant@consul01:/etc/consul.d$ And if we'd like to get detailed information for node consul01 using curl : curl --silent http://127.0.0.1:8500/v1/catalog/node/consul01 | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/node/consul01 | jq { \"Node\": { \"ID\": \"290c5be7-7a03-7e9a-5db4-d87f912c1674\", \"Node\": \"consul01\", \"Address\": \"192.168.250.11\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.11\", \"lan_ipv4\": \"192.168.250.11\", \"wan\": \"192.168.250.11\", \"wan_ipv4\": \"192.168.250.11\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 5, \"ModifyIndex\": 6 }, \"Services\": { \"consul\": { \"ID\": \"consul\", \"Service\": \"consul\", \"Tags\": [], \"Address\": \"\", \"Meta\": { \"raft_version\": \"3\", \"serf_protocol_current\": \"2\", \"serf_protocol_max\": \"5\", \"serf_protocol_min\": \"1\", \"version\": \"1.7.2\" }, \"Port\": 8300, \"Weights\": { \"Passing\": 1, \"Warning\": 1 }, \"EnableTagOverride\": false, \"Proxy\": { \"MeshGateway\": {}, \"Expose\": {} }, \"Connect\": {}, \"CreateIndex\": 5, \"ModifyIndex\": 5 } } } vagrant@consul01:/etc/consul.d$ We can also get the health of our consul01 node using curl : curl --silent http://127.0.0.1:8500/v1/health/node/consul01 | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/health/node/consul01 | jq [ { \"Node\": \"consul01\", \"CheckID\": \"serfHealth\", \"Name\": \"Serf Health Status\", \"Status\": \"passing\", \"Notes\": \"\", \"Output\": \"Agent alive and reachable\", \"ServiceID\": \"\", \"ServiceName\": \"\", \"ServiceTags\": [], \"Type\": \"\", \"Definition\": {}, \"CreateIndex\": 5, \"ModifyIndex\": 5 } ] vagrant@consul01:/etc/consul.d$","title":"CLI - Nodes"},{"location":"05_Consul/#cli-datacenter-members","text":"To get a list of Consul datacenter members, we can simply execute: consul members list vagrant@consul01:/etc/consul.d$ consul members list Node Address Status Type Build Protocol DC Segment consul01 192.168.250.11:8301 alive server 1.7.2 2 dc1 <all> vagrant@consul01:/etc/consul.d$ Next let's take a look at consul01 information specifically. consul info vagrant@consul01:/etc/consul.d$ consul info agent: check_monitors = 0 check_ttls = 0 checks = 0 services = 0 build: prerelease = revision = 9ea1a204 version = 1.7.2 consul: acl = disabled bootstrap = true known_datacenters = 1 leader = true leader_addr = 192.168.250.11:8300 server = true raft: applied_index = 142 commit_index = 142 fsm_pending = 0 last_contact = 0 last_log_index = 142 last_log_term = 3 last_snapshot_index = 0 last_snapshot_term = 0 latest_configuration = [{Suffrage:Voter ID:290c5be7-7a03-7e9a-5db4-d87f912c1674 Address:192.168.250.11:8300}] latest_configuration_index = 0 num_peers = 0 protocol_version = 3 protocol_version_max = 3 protocol_version_min = 0 snapshot_version_max = 1 snapshot_version_min = 0 state = Leader term = 3 runtime: arch = amd64 cpu_count = 1 goroutines = 81 max_procs = 2 os = linux version = go1.13.7 serf_lan: coordinate_resets = 0 encrypted = true event_queue = 1 event_time = 3 failed = 0 health_score = 0 intent_queue = 1 left = 0 member_time = 3 members = 1 query_queue = 0 query_time = 1 serf_wan: coordinate_resets = 0 encrypted = true event_queue = 0 event_time = 1 failed = 0 health_score = 0 intent_queue = 0 left = 0 member_time = 1 members = 1 query_queue = 0 query_time = 1 vagrant@consul01:/etc/consul.d$","title":"CLI - Datacenter Members"},{"location":"05_Consul/#ui","text":"Now that we've explored our Consul setup using some basic CLI commands, let's jump into the Consul UI. Open your preferred browser of choice and head over to http://192.168.250.11:8500/ui","title":"UI"},{"location":"05_Consul/#ui-services","text":"Click on services and you'll see we only have one service currently in Consul, which will look like below. And if you click on the Consul service to dig in further, you'll see a bit of information on our Consul service.","title":"UI - Services"},{"location":"05_Consul/#ui-nodes","text":"Next, let's click on nodes and we SHOULD only see our consul01 node listed. Let's now click on consul01 and dig in a bit more on this node.","title":"UI - Nodes"},{"location":"05_Consul/#dns","text":"One of the beautiful things with Consul is that we have access to a DNS interface. The DNS interfaces provides us with a way to perform various lookups using normal DNS methods. This removes the need to use the API to do a lookup of any kind. One thing to note here is: The Consul DNS interface listens on port 8600 by default. NOTE: We will explore options to get the DNS interface to become transparent to systems in a later excercise.","title":"DNS"},{"location":"05_Consul/#dns-node-lookups","text":"One example of using the DNS interface is to lookup nodes and get a list of records available. Let's see what DNS returns for our consul01 node. dig @127.0.0.1 -p 8600 consul01.node.consul ANY vagrant@consul01:/etc/consul.d$ dig @127.0.0.1 -p 8600 consul01.node.consul ANY ; <<>> DiG 9.11.3-1ubuntu1.11-Ubuntu <<>> @127.0.0.1 -p 8600 consul01.node.consul ANY ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 53553 ;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;consul01.node.consul. IN ANY ;; ANSWER SECTION: consul01.node.consul. 0 IN A 192.168.250.11 consul01.node.consul. 0 IN TXT \"consul-network-segment=\" ;; Query time: 1 msec ;; SERVER: 127.0.0.1#8600(127.0.0.1) ;; WHEN: Mon Jul 20 19:50:35 UTC 2020 ;; MSG SIZE rcvd: 101 vagrant@consul01:/etc/consul.d$","title":"DNS - Node Lookups"},{"location":"05_Consul/#dns-service-lookups","text":"We can also use Consul to perform service lookups. The context of a service lookup looks like: [tag.]<service>.service[.datacenter].<domain> . The tag is an optional field that can be used to find a specific tag when services may be named the same but are used for entirely different things. For example, we may have multiple db services registered. But we could use tags to delineate between development and production . So, any application for development could find it's db service by using development.db.service.consul . Also, the datacenter is optional unless you need to find a service outside of your Consul agent's default Consul datacenter. So, if you are looking for a service in the same Consul datacenter, you can skip the datacenter portion. Otherwise, you can add the datacenter to find a service in another Consul datacenter. Seeing as we don't have any services registered at this point, we will just use the default consul service to perform an example DNS service lookup. dig @127.0.0.1 -p 8600 consul.service.consul vagrant@consul01:/etc/consul.d$ dig @127.0.0.1 -p 8600 consul.service.consul ; <<>> DiG 9.11.3-1ubuntu1.11-Ubuntu <<>> @127.0.0.1 -p 8600 consul.service.consul ; (1 server found) ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 5946 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;consul.service.consul. IN A ;; ANSWER SECTION: consul.service.consul. 0 IN A 192.168.250.11 ;; Query time: 0 msec ;; SERVER: 127.0.0.1#8600(127.0.0.1) ;; WHEN: Mon Jul 20 20:06:08 UTC 2020 ;; MSG SIZE rcvd: 66 vagrant@consul01:/etc/consul.d$","title":"DNS - Service Lookups"},{"location":"05_Consul/#dns-systemd-resolved","text":"Oh my! systemd-resolved What a pain. However, we can work around this relatively easy and not disable it. Many times I have opted to just disable systemd-resolved and install Dnsmasq and move on. But for this learnining we will install Dnsmasq , reconfigure systemd-resolved to forward to Dnsmasq , and configure Dnsmasq to forward the consul domain to our Consul client's DNS port. We will be doing this on all of our servers within our stack to ensure they are all consistent. As well as, properly resolve services registered in Consul without the need to do specific work arounds for various things such as: NGINX, etc. The benefit will be to understand how we can leverge Consul in any environment. To easily reach our services without load balancers, etc. For example, we might have an NGINX web server configured as a reverse proxy for many different sites within our environment. Think of our NGINX reverse proxy functioning as a form of an application gateway. We could frontend our application gateways with an external load balancer, which sends all HTTP(s) traffic to our application gateways. Then our application gateways can make decisions based on the URL, to properly redirect to our applications. The point here is that we could easily just define resolvers for NGINX, but that wouldn't solve OS level DNS resolution for services such as logging, etc. So, what does this look like after we provision our servers? Let's jump in and take a look. vagrant ssh consul01 Next, let's look at our systemd-resolved configuration and then Dnsmasq. cat /etc/systemd/resolved.conf vagrant@consul01:~$ cat /etc/systemd/resolved.conf # This file is part of systemd. # # systemd is free software; you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. # # Entries in this file show the compile time defaults. # You can change settings by editing this file. # Defaults can be restored by simply deleting this file. # # See resolved.conf(5) for details [Resolve] DNS=127.0.0.2 #FallbackDNS= #Domains= #LLMNR=no #MulticastDNS=no #DNSSEC=no #Cache=yes #DNSStubListener=yes vagrant@consul01:~$ The important thing from above is the line DNS=127.0.0.2 . This is telling systemd-resolved to forward all lookups to 127.0.0.2 which is what IP Dnsmasq is bound to. And if we take a quick look at /etc/dnsmasq.conf we can verify that is correct as well. cat /etc/dnsmasq.conf vagrant@consul01:~$ cat /etc/dnsmasq.conf bind-interfaces port=53 listen-address=127.0.0.2 server=/consul/127.0.0.1#8600 # Uncomment and modify as appropriate to enable reverse DNS lookups for # common netblocks found in RFC 1918, 5735, and 6598: #rev-server=0.0.0.0/8,127.0.0.1#8600 #rev-server=10.0.0.0/8,127.0.0.1#8600 #rev-server=100.64.0.0/10,127.0.0.1#8600 #rev-server=127.0.0.1/8,127.0.0.1#8600 #rev-server=169.254.0.0/16,127.0.0.1#8600 #rev-server=172.16.0.0/12,127.0.0.1#8600 #rev-server=192.168.0.0/16,127.0.0.1#8600 #rev-server=224.0.0.0/4,127.0.0.1#8600 #rev-server=240.0.0.0/4,127.0.0.1#8600 vagrant@consul01:~$ And as you can see, we have configured listen-address=127.0.0.2 and bind-interfaces to ensure that Dnsmasq only listens on that IP. We have also added server=/consul/127.0.0.1#8600 which tells Dnsmasq to forward all consul domains to thee local Consul client's DNS port(8600). Now let's verify that our DNS resolution is working properly to find registered services in Consul. dig consul.service.dc1.consul vagrant@consul01:~$ dig consul.service.dc1.consul ; <<>> DiG 9.11.3-1ubuntu1.11-Ubuntu <<>> consul.service.dc1.consul ;; global options: +cmd ;; Got answer: ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 60558 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;consul.service.dc1.consul. IN A ;; ANSWER SECTION: consul.service.dc1.consul. 0 IN A 192.168.250.11 ;; Query time: 3 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Tue Jul 21 17:02:35 UTC 2020 ;; MSG SIZE rcvd: 70 vagrant@consul01:~$","title":"DNS - systemd-resolved"},{"location":"05_Consul/#kv-store","text":"The Consul KV store is another core feature of Consul. The KV store allows users to store indexed objects for various usages. The KV store is also a simple one that is not intended to replace a full featured KV datastore. The KV store does have an object size limitation of 512KB . Otherwise there are not any restrictions to its usage.","title":"KV Store"},{"location":"05_Consul/#kv-store-examples","text":"To get familiar using the KV store, we will walk through some basic examples.","title":"KV Store - Examples"},{"location":"05_Consul/#kv-store-examples-put","text":"First, let's create a few KV pairs: consul kv put users/example/username example vagrant@consul01:/etc/consul.d$ consul kv put users/example/username example Success! Data written to: users/example/username vagrant@consul01:/etc/consul.d$ consul kv put users/example/email example@example.org vagrant@consul01:/etc/consul.d$ consul kv put users/example/email example@example.org Success! Data written to: users/example/email vagrant@consul01:/etc/consul.d$ consul kv put users/example/lists [\"list1\",\"list2\"] vagrant@consul01:/etc/consul.d$ consul kv put users/example/lists [\"list1\",\"list2\"] Success! Data written to: users/example/lists vagrant@consul01:/etc/consul.d$ You can also add the contents of a file as the value for a key. So, let's use an example YAML file that is stored in our project. consul kv put cloud/config @/vagrant/learning/Consul/kv_example.yaml vagrant@consul01:/etc/consul.d$ consul kv put cloud/config @/vagrant/learning/Consul/kv_example.yaml Success! Data written to: cloud/config vagrant@consul01:/etc/consul.d$","title":"KV Store - Examples - Put"},{"location":"05_Consul/#kv-store-examples-get","text":"Now let's explore how we can get the KV pairs. consul kv get users/example/email vagrant@consul01:/etc/consul.d$ consul kv get users/example/email example@example.org vagrant@consul01:/etc/consul.d$ consul kv get users/example/lists vagrant@consul01:/etc/consul.d$ consul kv get users/example/lists [list1,list2] vagrant@consul01:/etc/consul.d$ We can also get all of the keys defined within a path recursively. consul kv get --recurse users vagrant@consul01:/etc/consul.d$ consul kv get --recurse users users/example/email:example@example.org users/example/lists:[list1,list2] users/example/username:example vagrant@consul01:/etc/consul.d$ Now let's get the contents of the file that we added. consul kv get cloud/config vagrant@consul01:/etc/consul.d$ consul kv get cloud/config # ================================================================= # Cloud Information # ================================================================= cloud: # Cloud Name: The cloud name must not contain spaces or special # characters. The name is used for the OpenStack region name. name: MyCloudName # Cloud Description description: Controller + N Compute Topology - x86 KVM # Cloud Administrator (admin) User's Password password: MyCloudPassword # Cloud Database Service Type: db2 or mysql database_service_type: db2 # Cloud Messaging Service Type: rabbitmq or qpid messaging_service_type: rabbitmq # Cloud Features: The cloud features to be enabled or disabled. features: self_service_portal: enabled platform_resource_scheduler: enabled # Cloud Topology: References the node name(s) for each role # within the cloud's topology. topology: database_node_name: controller controller_node_name: controller self_service_portal_node_name: controller kvm_compute_node_names: kvm_compute # ================================================================ # Environment Information # ================================================================ environment: base: example-ibm-os-single-controller-n-compute default_attributes: # (Optional) Add Default Environment Attributes override_attributes: # (Optional) Add Override Environment Attributes # ================================================================ # Node Information # ================================================================ nodes: - name: controller description: Cloud controller node fqdn: controllername.company.com password: passw0rd identity_file: ~ nics: management_network: eth0 data_network: eth1 - name: kvm_compute description: Cloud KVM compute node fqdn: kvmcomputename.company.com password: ~ identity_file: /root/identity.pem nics: management_network: eth0 data_network: eth1 # (Optional) Node Attribute JSON File attribute_file: ~ vagrant@consul01:/etc/consul.d$","title":"KV Store - Examples - Get"},{"location":"05_Consul/#kv-store-examples-delete","text":"Now that we've explored adding and getting KV pairs from Consul. Let's now explore deleting those KV pairs. consul kv delete users/example/lists vagrant@consul01:/etc/consul.d$ consul kv delete users/example/lists Success! Deleted key: users/example/lists vagrant@consul01:/etc/consul.d$ consul kv delete cloud/config vagrant@consul01:/etc/consul.d$ consul kv delete cloud/config Success! Deleted key: cloud/config vagrant@consul01:/etc/consul.d$ And as we saw previously with recursively getting keys, we can also delete keys recursively. consul kv delete --recurse users vagrant@consul01:/etc/consul.d$ consul kv delete --recurse users Success! Deleted keys with prefix: users vagrant@consul01:/etc/consul.d$","title":"KV Store - Examples - Delete"},{"location":"05_Consul/#tearing-down","text":"After you've explored a single Consul node setup. You'll likely be ready to move onto more advanced scenarios. So, just as we did when spinning up. We can quickly tear everything down. ./scripts/cleanup.sh \u25b6 ./scripts/cleanup.sh ==> consul01: Forcing shutdown of VM... ==> consul01: Destroying VM and associated drives... (venv)","title":"Tearing Down"},{"location":"06_Consul_Cluster/","text":"06. Consul - Cluster In this scenario, we will build upon a single Consul server and move onto setting up a three node Consul cluster. NOTE: For most of this setup, we will only cover the differences from our single node Consul server. Feel free to jump to 03. Consul to go through anything we covered previously. Spinning Up First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/consul_cluster.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'consul01' up with 'virtualbox' provider... Bringing machine 'consul02' up with 'virtualbox' provider... Bringing machine 'consul03' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* consul01 : ok=30 changed=13 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 consul02 : ok=30 changed=13 unreachable=0 failed=0 skipped=21 rescued=0 ignored=0 consul03 : ok=30 changed=13 unreachable=0 failed=0 skipped=21 rescued=0 ignored=0 And once everything completes, we will have a fully functional three node Consul cluster. This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11 consul02 192.168.250.12 consul03 192.168.250.13 CLI Now we are ready to begin exploring our Consul cluster using the CLI. So, to do this. Let's SSH into any one of our nodes consul0[1-3] . vagrant ssh consul01 \u25b6 vagrant ssh consul01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage Last login: Mon Jul 20 22:37:48 2020 from 192.168.250.1 vagrant@consul01:~$ Now that we've logged in, let's change into the /etc/consul.d directory and see what files/directories are present. cd /etc/consul.d ls -la vagrant@consul01:~$ cd /etc/consul.d vagrant@consul01:/etc/consul.d$ ls -la total 20 drwxr-xr-x 4 root root 4096 Jul 20 22:37 . drwxr-xr-x 79 root root 4096 Jul 20 22:37 .. drwxr-xr-x 2 root root 4096 Jul 20 22:37 client -rw-r--r-- 1 root root 898 Jul 20 22:37 config.json drwxr-xr-x 2 root root 4096 Jul 20 22:37 scripts vagrant@consul01:/etc/consul.d$ CLI - Configuration As we can see from above, our files and directories look identical to our single node Consul server. However, let's take a look at our config.json file. cat config.json vagrant@consul01:/etc/consul.d$ cat config.json { \"acl\": { \"default_policy\": \"allow\", \"down_policy\": \"extend-cache\", \"tokens\": { \"agent\": \"\", \"agent_master\": \"\", \"default\": \"\", \"master\": \"6DA12E0F-D8A5-48C5-AEFF-00D50E84D01A\", \"replication\": \"\" } }, \"bind_addr\": \"192.168.250.11\", \"bootstrap_expect\": 3, \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul\", \"datacenter\": \"dc1\", \"dns_config\": {}, \"enable_acl_replication\": false, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"consul01\", \"performance\": {}, \"primary_datacenter\": \"dc1\", \"retry_join\": [ \"192.168.250.11\", \"192.168.250.12\", \"192.168.250.13\" ], \"retry_join_wan\": [], \"server\": true, \"telemetry\": {}, \"ui\": true } vagrant@consul01:/etc/consul.d$ This configuration looks very much the same as our single node with the exception of: bootstrap_expect and retry_join . If you were to compare to our single node configuration you would see that these two settings are indeed different. How? CLI - Nodes Next, let's get a list of nodes available: consul catalog nodes vagrant@consul01:/etc/consul.d$ consul catalog nodes Node ID Address DC consul01 b70ec734 192.168.250.11 dc1 consul02 156f0b03 192.168.250.12 dc1 consul03 6040c3c2 192.168.250.13 dc1 vagrant@consul01:/etc/consul.d$ And as we did previously, let's get a more detailed view of our catalog nodes. consul catalog nodes --detailed vagrant@consul01:/etc/consul.d$ consul catalog nodes --detailed Node ID Address DC TaggedAddresses Meta consul01 b70ec734-8639-0a6b-cc95-8a71b63fb776 192.168.250.11 dc1 lan=192.168.250.11, lan_ipv4=192.168.250.11, wan=192.168.250.11, wan_ipv4=192.168.250.11 consul-network-segment= consul02 156f0b03-33db-7dfc-bb5b-3c682e51a815 192.168.250.12 dc1 lan=192.168.250.12, lan_ipv4=192.168.250.12, wan=192.168.250.12, wan_ipv4=192.168.250.12 consul-network-segment= consul03 6040c3c2-b2f8-c17e-ca75-873e83cb6455 192.168.250.13 dc1 lan=192.168.250.13, lan_ipv4=192.168.250.13, wan=192.168.250.13, wan_ipv4=192.168.250.13 consul-network-segment= vagrant@consul01:/etc/consul.d$ Let's also use curl to get a detailed view here as well. curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq [ { \"ID\": \"b70ec734-8639-0a6b-cc95-8a71b63fb776\", \"Node\": \"consul01\", \"Address\": \"192.168.250.11\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.11\", \"lan_ipv4\": \"192.168.250.11\", \"wan\": \"192.168.250.11\", \"wan_ipv4\": \"192.168.250.11\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 7, \"ModifyIndex\": 9 }, { \"ID\": \"156f0b03-33db-7dfc-bb5b-3c682e51a815\", \"Node\": \"consul02\", \"Address\": \"192.168.250.12\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.12\", \"lan_ipv4\": \"192.168.250.12\", \"wan\": \"192.168.250.12\", \"wan_ipv4\": \"192.168.250.12\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 5, \"ModifyIndex\": 8 }, { \"ID\": \"6040c3c2-b2f8-c17e-ca75-873e83cb6455\", \"Node\": \"consul03\", \"Address\": \"192.168.250.13\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.13\", \"lan_ipv4\": \"192.168.250.13\", \"wan\": \"192.168.250.13\", \"wan_ipv4\": \"192.168.250.13\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 6, \"ModifyIndex\": 10 } ] vagrant@consul01:/etc/consul.d$ CLI - Datacenter Members To get a list of Consul datacenter members, we can simply execute: consul members list vagrant@consul01:/etc/consul.d$ consul members list Node Address Status Type Build Protocol DC Segment consul01 192.168.250.11:8301 alive server 1.7.2 2 dc1 <all> consul02 192.168.250.12:8301 alive server 1.7.2 2 dc1 <all> consul03 192.168.250.13:8301 alive server 1.7.2 2 dc1 <all> vagrant@consul01:/etc/consul.d$ UI - Services Click on services and you'll see we only have one service currently in Consul, which will look like below. And if you click on the Consul service to dig in further, you'll see a bit of information on our Consul service. UI - Nodes Next, let's click on nodes and we SHOULD now see our three nodes. NOTE: At this point everything else SHOULD work the exact same as it did with our single node Consul server. And as mentioned previously, feel free to explore those different things we did in 03. Consul . Tearing Down After you've explored a Consul cluster setup. You'll likely be ready to move onto more advanced scenarios. So, just as we did when spinning up. We can quickly tear everything down. ./scripts/cleanup.sh \u25b6 ./scripts/cleanup.sh ==> consul03: Forcing shutdown of VM... ==> consul03: Destroying VM and associated drives... ==> consul02: Forcing shutdown of VM... ==> consul02: Destroying VM and associated drives... ==> consul01: Forcing shutdown of VM... ==> consul01: Destroying VM and associated drives... (venv)","title":"06. Consul - Cluster"},{"location":"06_Consul_Cluster/#06-consul-cluster","text":"In this scenario, we will build upon a single Consul server and move onto setting up a three node Consul cluster. NOTE: For most of this setup, we will only cover the differences from our single node Consul server. Feel free to jump to 03. Consul to go through anything we covered previously.","title":"06. Consul - Cluster"},{"location":"06_Consul_Cluster/#spinning-up","text":"First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/consul_cluster.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'consul01' up with 'virtualbox' provider... Bringing machine 'consul02' up with 'virtualbox' provider... Bringing machine 'consul03' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* consul01 : ok=30 changed=13 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 consul02 : ok=30 changed=13 unreachable=0 failed=0 skipped=21 rescued=0 ignored=0 consul03 : ok=30 changed=13 unreachable=0 failed=0 skipped=21 rescued=0 ignored=0 And once everything completes, we will have a fully functional three node Consul cluster. This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11 consul02 192.168.250.12 consul03 192.168.250.13","title":"Spinning Up"},{"location":"06_Consul_Cluster/#cli","text":"Now we are ready to begin exploring our Consul cluster using the CLI. So, to do this. Let's SSH into any one of our nodes consul0[1-3] . vagrant ssh consul01 \u25b6 vagrant ssh consul01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage Last login: Mon Jul 20 22:37:48 2020 from 192.168.250.1 vagrant@consul01:~$ Now that we've logged in, let's change into the /etc/consul.d directory and see what files/directories are present. cd /etc/consul.d ls -la vagrant@consul01:~$ cd /etc/consul.d vagrant@consul01:/etc/consul.d$ ls -la total 20 drwxr-xr-x 4 root root 4096 Jul 20 22:37 . drwxr-xr-x 79 root root 4096 Jul 20 22:37 .. drwxr-xr-x 2 root root 4096 Jul 20 22:37 client -rw-r--r-- 1 root root 898 Jul 20 22:37 config.json drwxr-xr-x 2 root root 4096 Jul 20 22:37 scripts vagrant@consul01:/etc/consul.d$","title":"CLI"},{"location":"06_Consul_Cluster/#cli-configuration","text":"As we can see from above, our files and directories look identical to our single node Consul server. However, let's take a look at our config.json file. cat config.json vagrant@consul01:/etc/consul.d$ cat config.json { \"acl\": { \"default_policy\": \"allow\", \"down_policy\": \"extend-cache\", \"tokens\": { \"agent\": \"\", \"agent_master\": \"\", \"default\": \"\", \"master\": \"6DA12E0F-D8A5-48C5-AEFF-00D50E84D01A\", \"replication\": \"\" } }, \"bind_addr\": \"192.168.250.11\", \"bootstrap_expect\": 3, \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul\", \"datacenter\": \"dc1\", \"dns_config\": {}, \"enable_acl_replication\": false, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"consul01\", \"performance\": {}, \"primary_datacenter\": \"dc1\", \"retry_join\": [ \"192.168.250.11\", \"192.168.250.12\", \"192.168.250.13\" ], \"retry_join_wan\": [], \"server\": true, \"telemetry\": {}, \"ui\": true } vagrant@consul01:/etc/consul.d$ This configuration looks very much the same as our single node with the exception of: bootstrap_expect and retry_join . If you were to compare to our single node configuration you would see that these two settings are indeed different. How?","title":"CLI - Configuration"},{"location":"06_Consul_Cluster/#cli-nodes","text":"Next, let's get a list of nodes available: consul catalog nodes vagrant@consul01:/etc/consul.d$ consul catalog nodes Node ID Address DC consul01 b70ec734 192.168.250.11 dc1 consul02 156f0b03 192.168.250.12 dc1 consul03 6040c3c2 192.168.250.13 dc1 vagrant@consul01:/etc/consul.d$ And as we did previously, let's get a more detailed view of our catalog nodes. consul catalog nodes --detailed vagrant@consul01:/etc/consul.d$ consul catalog nodes --detailed Node ID Address DC TaggedAddresses Meta consul01 b70ec734-8639-0a6b-cc95-8a71b63fb776 192.168.250.11 dc1 lan=192.168.250.11, lan_ipv4=192.168.250.11, wan=192.168.250.11, wan_ipv4=192.168.250.11 consul-network-segment= consul02 156f0b03-33db-7dfc-bb5b-3c682e51a815 192.168.250.12 dc1 lan=192.168.250.12, lan_ipv4=192.168.250.12, wan=192.168.250.12, wan_ipv4=192.168.250.12 consul-network-segment= consul03 6040c3c2-b2f8-c17e-ca75-873e83cb6455 192.168.250.13 dc1 lan=192.168.250.13, lan_ipv4=192.168.250.13, wan=192.168.250.13, wan_ipv4=192.168.250.13 consul-network-segment= vagrant@consul01:/etc/consul.d$ Let's also use curl to get a detailed view here as well. curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq vagrant@consul01:/etc/consul.d$ curl --silent http://127.0.0.1:8500/v1/catalog/nodes | jq [ { \"ID\": \"b70ec734-8639-0a6b-cc95-8a71b63fb776\", \"Node\": \"consul01\", \"Address\": \"192.168.250.11\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.11\", \"lan_ipv4\": \"192.168.250.11\", \"wan\": \"192.168.250.11\", \"wan_ipv4\": \"192.168.250.11\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 7, \"ModifyIndex\": 9 }, { \"ID\": \"156f0b03-33db-7dfc-bb5b-3c682e51a815\", \"Node\": \"consul02\", \"Address\": \"192.168.250.12\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.12\", \"lan_ipv4\": \"192.168.250.12\", \"wan\": \"192.168.250.12\", \"wan_ipv4\": \"192.168.250.12\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 5, \"ModifyIndex\": 8 }, { \"ID\": \"6040c3c2-b2f8-c17e-ca75-873e83cb6455\", \"Node\": \"consul03\", \"Address\": \"192.168.250.13\", \"Datacenter\": \"dc1\", \"TaggedAddresses\": { \"lan\": \"192.168.250.13\", \"lan_ipv4\": \"192.168.250.13\", \"wan\": \"192.168.250.13\", \"wan_ipv4\": \"192.168.250.13\" }, \"Meta\": { \"consul-network-segment\": \"\" }, \"CreateIndex\": 6, \"ModifyIndex\": 10 } ] vagrant@consul01:/etc/consul.d$","title":"CLI - Nodes"},{"location":"06_Consul_Cluster/#cli-datacenter-members","text":"To get a list of Consul datacenter members, we can simply execute: consul members list vagrant@consul01:/etc/consul.d$ consul members list Node Address Status Type Build Protocol DC Segment consul01 192.168.250.11:8301 alive server 1.7.2 2 dc1 <all> consul02 192.168.250.12:8301 alive server 1.7.2 2 dc1 <all> consul03 192.168.250.13:8301 alive server 1.7.2 2 dc1 <all> vagrant@consul01:/etc/consul.d$","title":"CLI - Datacenter Members"},{"location":"06_Consul_Cluster/#ui-services","text":"Click on services and you'll see we only have one service currently in Consul, which will look like below. And if you click on the Consul service to dig in further, you'll see a bit of information on our Consul service.","title":"UI - Services"},{"location":"06_Consul_Cluster/#ui-nodes","text":"Next, let's click on nodes and we SHOULD now see our three nodes. NOTE: At this point everything else SHOULD work the exact same as it did with our single node Consul server. And as mentioned previously, feel free to explore those different things we did in 03. Consul .","title":"UI - Nodes"},{"location":"06_Consul_Cluster/#tearing-down","text":"After you've explored a Consul cluster setup. You'll likely be ready to move onto more advanced scenarios. So, just as we did when spinning up. We can quickly tear everything down. ./scripts/cleanup.sh \u25b6 ./scripts/cleanup.sh ==> consul03: Forcing shutdown of VM... ==> consul03: Destroying VM and associated drives... ==> consul02: Forcing shutdown of VM... ==> consul02: Destroying VM and associated drives... ==> consul01: Forcing shutdown of VM... ==> consul01: Destroying VM and associated drives... (venv)","title":"Tearing Down"},{"location":"07_Consul_Service_Discovery_HAProxy/","text":"07. Consul - Service Discovery with HAProxy In this scenario, we will be spinning up the following: a single Consul node, three app servers, and a load balancer. NOTE: This scenario was borrowed from https://learn.hashicorp.com/consul/integrations/haproxy-consul . However, we've automated the whole process. Spinning Up First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/consul_service_discovery_haproxy.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'consul01' up with 'virtualbox' provider... Bringing machine 'app01' up with 'virtualbox' provider... Bringing machine 'app02' up with 'virtualbox' provider... Bringing machine 'app03' up with 'virtualbox' provider... Bringing machine 'lb01' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* app01 : ok=51 changed=27 unreachable=0 failed=0 skipped=28 rescued=0 ignored=0 app02 : ok=51 changed=27 unreachable=0 failed=0 skipped=28 rescued=0 ignored=0 app03 : ok=51 changed=27 unreachable=0 failed=0 skipped=28 rescued=0 ignored=0 consul01 : ok=38 changed=19 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 lb01 : ok=54 changed=30 unreachable=0 failed=0 skipped=10 rescued=0 ignored=0 And once everything completes, we will have a fully functional Consul service discovery with HAProxy. Nodes This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11 app01 192.168.250.21 app02 192.168.250.22 app03 192.168.250.23 lb01 192.168.250.101 192.168.250.201 HAProxy Service Discovery To get a quick view of the HAProxy admin stats page, head over here and login with admin:admin . And as you can see, we have some web servers up, running and fully integrated with Consul using service discovery. Let's quickly SSH into our HAProxy server and take a look at the Consul and HAProxy configurations. vagrant ssh lb01 \u25b6 vagrant ssh lb01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage * \"If you've been waiting for the perfect Kubernetes dev solution for macOS, the wait is over. Learn how to install Microk8s on macOS.\" https://www.techrepublic.com/article/how-to-install-microk8s-on-macos/ Last login: Tue Jul 21 20:00:52 2020 from 192.168.250.1 vagrant@lb01:~$ Now that we've logged in, let's change to the /etc/consul.d/client directory. cd /etc/consul.d/client Next let's look at the Consul client configuration. cat config.json vagrant@lb01:/etc/consul.d/client$ cat config.json { \"bind_addr\": \"192.168.250.101\", \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul/data\", \"datacenter\": \"dc1\", \"enable_script_checks\": true, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"lb01\", \"retry_join\": [ \"192.168.250.11\" ], \"server\": false, \"ui\": true } vagrant@lb01:/etc/consul.d/client$ And as you can see in the above configuration, we are using Consul as a client rather than a server ( \"server\": false ). Next let's change to the /etc/haproxy directory and look at our HAProxy configuration. cd /etc/haproxy cat haproxy.cfg vagrant@lb01:/etc/haproxy$ cat haproxy.cfg # # Ansible managed # global log /dev/log local0 log /dev/log local1 notice daemon chroot /var/lib/haproxy group haproxy maxconn 40000 spread-checks 3 stats socket /var/run/haproxy.sock mode 660 level admin stats timeout 30s user haproxy # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). ssl-default-bind-ciphers kEECDH+aRSA+AES:kRSA+AES:+AES256:RC4-SHA:!kEDH:!LOW:!EXP:!MD5:!aNULL:!eNULL ssl-default-bind-options no-sslv3 defaults log global maxconn 40000 mode tcp option dontlognull option redispatch option tcp-smart-accept option tcp-smart-connect option tcplog retries 3 timeout client 50000 timeout connect 50000 timeout queue 5000 timeout server 50000 userlist STATSUSERS group admin users admin user admin insecure-password admin frontend stats acl AuthOkay_ReadOnly http_auth(STATSUSERS) acl AuthOkay_Admin http_auth_group(STATSUSERS) admin bind *:9090 mode http stats enable stats http-request auth realm stats unless AuthOkay_ReadOnly stats refresh 10s stats show-legends stats uri / frontend http_front-80 bind 192.168.250.101:80 default_backend http_back backend http_back balance roundrobin server-template mywebapp 10 _web._tcp.service.consul resolvers consul resolve-opts allow-dup-ip resolve-prefer ipv4 check resolvers consul nameserver consul 127.0.0.1:8600 accepted_payload_size 8192 hold valid 5s vagrant@lb01:/etc/haproxy$ Now looking at this configuration, the real magic is happening within the following two blocks: backend http_back balance roundrobin server-template mywebapp 10 _web._tcp.service.consul resolvers consul resolve-opts allow-dup-ip resolve-prefer ipv4 check The above block is defining the server-template which is used to perform the service discovery within our Consul cluster. The template is telling HAProxy to discover the web services that are registered in Consul using the consul resolver as defined below. To do the DNS discovery for us and return the results back to HAProxy. resolvers consul nameserver consul 127.0.0.1:8600 accepted_payload_size 8192 hold valid 5s App Servers We have also spun up three app servers with our stack. These servers will over time provide us many different services in which we will use for learning. But for now, the following list is what they provide: NGINX: HTTP NOTE: SSH to any one of app[01-03] . These servers are also running the Consul client which is configured as below: vagrant ssh app01 cd /etc/consul.d/client cat config.json vagrant@app01:/etc/consul.d/client$ cat config.json { \"bind_addr\": \"192.168.250.31\", \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul/data\", \"datacenter\": \"dc1\", \"enable_script_checks\": true, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"app01\", \"retry_join\": [ \"192.168.250.11\" ], \"server\": false, \"ui\": true } vagrant@app01:/etc/consul.d/client$ And once again, we SHOULD notice that our Consul client configuration is identical to our HAProxy server. Our Consul services that we are registering, are being done using Ansible. The following shows how that is accomplished for us. We could also do this creating Consul client configurations on our app servers, but we've chosen to use Ansible. group_vars/app_servers/consul.yml --- # Define Consul services for app servers consul_services: - name: grafana port: 3000 script: curl --silent http://localhost:3000/api/health interval: 3s state: present - name: web port: 80 script: curl --silent http://localhost interval: 3s state: present playbooks/app_servers.yml - hosts: app_servers tasks: - name: Manage Consul Services consul: service_name: \"{{ item.name }}\" service_port: \"{{ item.port }}\" script: \"{{ item.script|default(omit) }}\" interval: \"{{ item.interval|default(omit) }}\" state: \"{{ item.state|default(omit) }}\" become: true loop: \"{{ consul_services }}\" And to prove that they are working correctly through our HAProxy load balancer, simply browse to http://192.168.250.101 .","title":"07. Consul - Service Discovery with HAProxy"},{"location":"07_Consul_Service_Discovery_HAProxy/#07-consul-service-discovery-with-haproxy","text":"In this scenario, we will be spinning up the following: a single Consul node, three app servers, and a load balancer. NOTE: This scenario was borrowed from https://learn.hashicorp.com/consul/integrations/haproxy-consul . However, we've automated the whole process.","title":"07. Consul - Service Discovery with HAProxy"},{"location":"07_Consul_Service_Discovery_HAProxy/#spinning-up","text":"First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/consul_service_discovery_haproxy.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'consul01' up with 'virtualbox' provider... Bringing machine 'app01' up with 'virtualbox' provider... Bringing machine 'app02' up with 'virtualbox' provider... Bringing machine 'app03' up with 'virtualbox' provider... Bringing machine 'lb01' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* app01 : ok=51 changed=27 unreachable=0 failed=0 skipped=28 rescued=0 ignored=0 app02 : ok=51 changed=27 unreachable=0 failed=0 skipped=28 rescued=0 ignored=0 app03 : ok=51 changed=27 unreachable=0 failed=0 skipped=28 rescued=0 ignored=0 consul01 : ok=38 changed=19 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 lb01 : ok=54 changed=30 unreachable=0 failed=0 skipped=10 rescued=0 ignored=0 And once everything completes, we will have a fully functional Consul service discovery with HAProxy.","title":"Spinning Up"},{"location":"07_Consul_Service_Discovery_HAProxy/#nodes","text":"This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11 app01 192.168.250.21 app02 192.168.250.22 app03 192.168.250.23 lb01 192.168.250.101 192.168.250.201","title":"Nodes"},{"location":"07_Consul_Service_Discovery_HAProxy/#haproxy-service-discovery","text":"To get a quick view of the HAProxy admin stats page, head over here and login with admin:admin . And as you can see, we have some web servers up, running and fully integrated with Consul using service discovery. Let's quickly SSH into our HAProxy server and take a look at the Consul and HAProxy configurations. vagrant ssh lb01 \u25b6 vagrant ssh lb01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage * \"If you've been waiting for the perfect Kubernetes dev solution for macOS, the wait is over. Learn how to install Microk8s on macOS.\" https://www.techrepublic.com/article/how-to-install-microk8s-on-macos/ Last login: Tue Jul 21 20:00:52 2020 from 192.168.250.1 vagrant@lb01:~$ Now that we've logged in, let's change to the /etc/consul.d/client directory. cd /etc/consul.d/client Next let's look at the Consul client configuration. cat config.json vagrant@lb01:/etc/consul.d/client$ cat config.json { \"bind_addr\": \"192.168.250.101\", \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul/data\", \"datacenter\": \"dc1\", \"enable_script_checks\": true, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"lb01\", \"retry_join\": [ \"192.168.250.11\" ], \"server\": false, \"ui\": true } vagrant@lb01:/etc/consul.d/client$ And as you can see in the above configuration, we are using Consul as a client rather than a server ( \"server\": false ). Next let's change to the /etc/haproxy directory and look at our HAProxy configuration. cd /etc/haproxy cat haproxy.cfg vagrant@lb01:/etc/haproxy$ cat haproxy.cfg # # Ansible managed # global log /dev/log local0 log /dev/log local1 notice daemon chroot /var/lib/haproxy group haproxy maxconn 40000 spread-checks 3 stats socket /var/run/haproxy.sock mode 660 level admin stats timeout 30s user haproxy # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). ssl-default-bind-ciphers kEECDH+aRSA+AES:kRSA+AES:+AES256:RC4-SHA:!kEDH:!LOW:!EXP:!MD5:!aNULL:!eNULL ssl-default-bind-options no-sslv3 defaults log global maxconn 40000 mode tcp option dontlognull option redispatch option tcp-smart-accept option tcp-smart-connect option tcplog retries 3 timeout client 50000 timeout connect 50000 timeout queue 5000 timeout server 50000 userlist STATSUSERS group admin users admin user admin insecure-password admin frontend stats acl AuthOkay_ReadOnly http_auth(STATSUSERS) acl AuthOkay_Admin http_auth_group(STATSUSERS) admin bind *:9090 mode http stats enable stats http-request auth realm stats unless AuthOkay_ReadOnly stats refresh 10s stats show-legends stats uri / frontend http_front-80 bind 192.168.250.101:80 default_backend http_back backend http_back balance roundrobin server-template mywebapp 10 _web._tcp.service.consul resolvers consul resolve-opts allow-dup-ip resolve-prefer ipv4 check resolvers consul nameserver consul 127.0.0.1:8600 accepted_payload_size 8192 hold valid 5s vagrant@lb01:/etc/haproxy$ Now looking at this configuration, the real magic is happening within the following two blocks: backend http_back balance roundrobin server-template mywebapp 10 _web._tcp.service.consul resolvers consul resolve-opts allow-dup-ip resolve-prefer ipv4 check The above block is defining the server-template which is used to perform the service discovery within our Consul cluster. The template is telling HAProxy to discover the web services that are registered in Consul using the consul resolver as defined below. To do the DNS discovery for us and return the results back to HAProxy. resolvers consul nameserver consul 127.0.0.1:8600 accepted_payload_size 8192 hold valid 5s","title":"HAProxy Service Discovery"},{"location":"07_Consul_Service_Discovery_HAProxy/#app-servers","text":"We have also spun up three app servers with our stack. These servers will over time provide us many different services in which we will use for learning. But for now, the following list is what they provide: NGINX: HTTP NOTE: SSH to any one of app[01-03] . These servers are also running the Consul client which is configured as below: vagrant ssh app01 cd /etc/consul.d/client cat config.json vagrant@app01:/etc/consul.d/client$ cat config.json { \"bind_addr\": \"192.168.250.31\", \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/var/consul/data\", \"datacenter\": \"dc1\", \"enable_script_checks\": true, \"enable_syslog\": true, \"encrypt\": \"WWw4l0h1LbB4+pC5+VUWiV8kMBNQc+nEwt8OODMx2xg=\", \"log_level\": \"DEBUG\", \"node_name\": \"app01\", \"retry_join\": [ \"192.168.250.11\" ], \"server\": false, \"ui\": true } vagrant@app01:/etc/consul.d/client$ And once again, we SHOULD notice that our Consul client configuration is identical to our HAProxy server. Our Consul services that we are registering, are being done using Ansible. The following shows how that is accomplished for us. We could also do this creating Consul client configurations on our app servers, but we've chosen to use Ansible. group_vars/app_servers/consul.yml --- # Define Consul services for app servers consul_services: - name: grafana port: 3000 script: curl --silent http://localhost:3000/api/health interval: 3s state: present - name: web port: 80 script: curl --silent http://localhost interval: 3s state: present playbooks/app_servers.yml - hosts: app_servers tasks: - name: Manage Consul Services consul: service_name: \"{{ item.name }}\" service_port: \"{{ item.port }}\" script: \"{{ item.script|default(omit) }}\" interval: \"{{ item.interval|default(omit) }}\" state: \"{{ item.state|default(omit) }}\" become: true loop: \"{{ consul_services }}\" And to prove that they are working correctly through our HAProxy load balancer, simply browse to http://192.168.250.101 .","title":"App Servers"},{"location":"08_Consul_NGINX_Reverse_Proxy/","text":"08. Consul - NGINX Reverse Proxy","title":"08. Consul - NGINX Reverse Proxy"},{"location":"08_Consul_NGINX_Reverse_Proxy/#08-consul-nginx-reverse-proxy","text":"","title":"08. Consul - NGINX Reverse Proxy"},{"location":"09_Vault/","text":"09. Vault - Single Node In this scenario, we will simply spin up a single node Vault server which will provide us a learning environment to get familiar with basic Vault concepts. Spinning Up First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/vault.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'vault01' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* vault01 : ok=44 changed=14 unreachable=0 failed=0 skipped=23 rescued=0 ignored=0 Nodes This scenario has the following nodes when completed. Node IP #1 IP #2 vault01 192.168.250.21 CLI Now we are ready to begin exploring our Vault server using the CLI. So, to do this. Let's SSH into our vault01 node. vagrant ssh vault01 \u25b6 vagrant ssh vault01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage Last login: Wed Jul 22 02:01:12 2020 from 10.0.2.2 vagrant@vault01:~$ Now that we've logged in, let's change into the /etc/vault.d directory and see what files/directories are present. cd /etc/vault.d ls -la vagrant@vault01:~$ cd /etc/vault.d vagrant@vault01:/etc/vault.d$ ls -la total 16 drwxr-xr-x 2 root root 4096 Jul 22 02:00 . drwxr-xr-x 79 root root 4096 Jul 22 02:00 .. -r-------- 1 root root 768 Jul 22 02:00 .hashi_vault_init.json -rw-r--r-- 1 root vault 368 Jul 22 02:00 vault-config.json vagrant@vault01:/etc/vault.d$ CLI - Configuration NOTE: Vault is in an unsealed state at the time of spinning up. As part of the provisioning, our Vault keys are stored on each of the Vault servers (Not the safest, but...). So, let's see how we can get access to these. In our /etc/vault.d directory there is a hidden file .hashi_vault_init.json with only read permissions for the root user. So, in order to view the keys we will need to use sudo . sudo cat .hashi_vault_init.json vagrant@vault01:/etc/vault.d$ sudo cat .hashi_vault_init.json { \"keys\": [ \"1d110968b43499898b919912af59f1debf107304c6273b343dcff41c510cdd4929\", \"51a7f642c73c49354c004d77736e9dd7b889b532f339c939d7f2490e3768afbbee\", \"fbaf4298ad4a93f7045a98829f2623c6b9b5a4cd580ccbaf50b07344a3374ebb9c\", \"a95dd1c8b96d9cd11b49fc3ebebc92161706c959d4d36c8e5180390eadf46fa261\", \"074af393c1bfe94052140561591b14f948705e7a129bbc9510e1e01a042487853e\" ], \"keys_base64\": [ \"HREJaLQ0mYmLkZkSr1nx3r8QcwTGJzs0Pc/0HFEM3Ukp\", \"Uaf2Qsc8STVMAE13c26d17iJtTLzOck51/JJDjdor7vu\", \"+69CmK1Kk/cEWpiCnyYjxrm1pM1YDMuvULBzRKM3Truc\", \"qV3RyLltnNEbSfw+vrySFhcGyVnU02yOUYA5Dq30b6Jh\", \"B0rzk8G/6UBSFAVhWRsU+UhwXnoSm7yVEOHgGgQkh4U+\" ], \"root_token\": \"s.Epa2crdqGBbe9LpOLkJtuwJG\" } vagrant@vault01:/etc/vault.d$ Let's first take a quick look at our Vault configuration while we're here. cat vault-config.json vagrant@vault01:/etc/vault.d$ cat vault-config.json { \"api_addr\": \"http://192.168.250.21:8200\", \"listener\": [ { \"tcp\": { \"address\": \"127.0.0.1:8200\", \"cluster_address\": \"127.0.0.1:8201\", \"tls_disable\": true } } ], \"storage\": { \"file\": { \"path\": \"/var/lib/vault\" } }, \"ui\": true } vagrant@vault01:/etc/vault.d$ CLI - Status Now that we've checked out our Vault configuration, let's check the status. First we will need to set an environment variable to define our VAULT_ADDR . This is required because we are not using TLS, which is the default for the vault command. export VAULT_ADDR=\"http://127.0.0.1:8200\" Now we can check our Vault status properly from the CLI. vault status vagrant@vault01:/etc/vault.d$ vault status Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.4.1 Cluster Name vault-cluster-1bd459aa Cluster ID 8041b47b-edc0-6f9e-4360-9999e3f4640e HA Enabled false vagrant@vault01:/etc/vault.d$ Tearing Down After you've explored a single Vault node setup. You'll likely be ready to move onto more advanced scenarios. So, just as we did when spinning up. We can quickly tear everything down. ./scripts/cleanup.sh \u25b6 ./scripts/cleanup.sh ==> vault01: Forcing shutdown of VM... ==> vault01: Destroying VM and associated drives... (venv)","title":"09. Vault - Single Node"},{"location":"09_Vault/#09-vault-single-node","text":"In this scenario, we will simply spin up a single node Vault server which will provide us a learning environment to get familiar with basic Vault concepts.","title":"09. Vault - Single Node"},{"location":"09_Vault/#spinning-up","text":"First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/vault.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. \u25b6 vagrant up Bringing machine 'vault01' up with 'virtualbox' provider... ... PLAY RECAP ********************************************************************* vault01 : ok=44 changed=14 unreachable=0 failed=0 skipped=23 rescued=0 ignored=0","title":"Spinning Up"},{"location":"09_Vault/#nodes","text":"This scenario has the following nodes when completed. Node IP #1 IP #2 vault01 192.168.250.21","title":"Nodes"},{"location":"09_Vault/#cli","text":"Now we are ready to begin exploring our Vault server using the CLI. So, to do this. Let's SSH into our vault01 node. vagrant ssh vault01 \u25b6 vagrant ssh vault01 Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-76-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantage Last login: Wed Jul 22 02:01:12 2020 from 10.0.2.2 vagrant@vault01:~$ Now that we've logged in, let's change into the /etc/vault.d directory and see what files/directories are present. cd /etc/vault.d ls -la vagrant@vault01:~$ cd /etc/vault.d vagrant@vault01:/etc/vault.d$ ls -la total 16 drwxr-xr-x 2 root root 4096 Jul 22 02:00 . drwxr-xr-x 79 root root 4096 Jul 22 02:00 .. -r-------- 1 root root 768 Jul 22 02:00 .hashi_vault_init.json -rw-r--r-- 1 root vault 368 Jul 22 02:00 vault-config.json vagrant@vault01:/etc/vault.d$","title":"CLI"},{"location":"09_Vault/#cli-configuration","text":"NOTE: Vault is in an unsealed state at the time of spinning up. As part of the provisioning, our Vault keys are stored on each of the Vault servers (Not the safest, but...). So, let's see how we can get access to these. In our /etc/vault.d directory there is a hidden file .hashi_vault_init.json with only read permissions for the root user. So, in order to view the keys we will need to use sudo . sudo cat .hashi_vault_init.json vagrant@vault01:/etc/vault.d$ sudo cat .hashi_vault_init.json { \"keys\": [ \"1d110968b43499898b919912af59f1debf107304c6273b343dcff41c510cdd4929\", \"51a7f642c73c49354c004d77736e9dd7b889b532f339c939d7f2490e3768afbbee\", \"fbaf4298ad4a93f7045a98829f2623c6b9b5a4cd580ccbaf50b07344a3374ebb9c\", \"a95dd1c8b96d9cd11b49fc3ebebc92161706c959d4d36c8e5180390eadf46fa261\", \"074af393c1bfe94052140561591b14f948705e7a129bbc9510e1e01a042487853e\" ], \"keys_base64\": [ \"HREJaLQ0mYmLkZkSr1nx3r8QcwTGJzs0Pc/0HFEM3Ukp\", \"Uaf2Qsc8STVMAE13c26d17iJtTLzOck51/JJDjdor7vu\", \"+69CmK1Kk/cEWpiCnyYjxrm1pM1YDMuvULBzRKM3Truc\", \"qV3RyLltnNEbSfw+vrySFhcGyVnU02yOUYA5Dq30b6Jh\", \"B0rzk8G/6UBSFAVhWRsU+UhwXnoSm7yVEOHgGgQkh4U+\" ], \"root_token\": \"s.Epa2crdqGBbe9LpOLkJtuwJG\" } vagrant@vault01:/etc/vault.d$ Let's first take a quick look at our Vault configuration while we're here. cat vault-config.json vagrant@vault01:/etc/vault.d$ cat vault-config.json { \"api_addr\": \"http://192.168.250.21:8200\", \"listener\": [ { \"tcp\": { \"address\": \"127.0.0.1:8200\", \"cluster_address\": \"127.0.0.1:8201\", \"tls_disable\": true } } ], \"storage\": { \"file\": { \"path\": \"/var/lib/vault\" } }, \"ui\": true } vagrant@vault01:/etc/vault.d$","title":"CLI - Configuration"},{"location":"09_Vault/#cli-status","text":"Now that we've checked out our Vault configuration, let's check the status. First we will need to set an environment variable to define our VAULT_ADDR . This is required because we are not using TLS, which is the default for the vault command. export VAULT_ADDR=\"http://127.0.0.1:8200\" Now we can check our Vault status properly from the CLI. vault status vagrant@vault01:/etc/vault.d$ vault status Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.4.1 Cluster Name vault-cluster-1bd459aa Cluster ID 8041b47b-edc0-6f9e-4360-9999e3f4640e HA Enabled false vagrant@vault01:/etc/vault.d$","title":"CLI - Status"},{"location":"09_Vault/#tearing-down","text":"After you've explored a single Vault node setup. You'll likely be ready to move onto more advanced scenarios. So, just as we did when spinning up. We can quickly tear everything down. ./scripts/cleanup.sh \u25b6 ./scripts/cleanup.sh ==> vault01: Forcing shutdown of VM... ==> vault01: Destroying VM and associated drives... (venv)","title":"Tearing Down"},{"location":"10_Vault_HA/","text":"10. Vault - HA In this scenario, we will spin up a three node Consul cluster along with three Vault servers configured in HA. This scenario is perfect to learn a bit about how Vault and Consul function together in HA. Spinning Up First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/vault_ha.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job. Nodes This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11 consul02 192.168.250.12 consul03 192.168.250.13 vault01 192.168.250.21 vault02 192.168.250.22 vault03 192.168.250.23","title":"10. Vault - HA"},{"location":"10_Vault_HA/#10-vault-ha","text":"In this scenario, we will spin up a three node Consul cluster along with three Vault servers configured in HA. This scenario is perfect to learn a bit about how Vault and Consul function together in HA.","title":"10. Vault - HA"},{"location":"10_Vault_HA/#spinning-up","text":"First we need to export our scenario configuration for Vagrant: export SCENARIO=scenarios/vault_ha.yml Now that we've exported our scenario configuration, we are ready to spin up our environment: vagrant up And off we go! You will see a lot going on here as Vagrant and Ansible do their job.","title":"Spinning Up"},{"location":"10_Vault_HA/#nodes","text":"This scenario has the following nodes when completed. Node IP #1 IP #2 consul01 192.168.250.11 consul02 192.168.250.12 consul03 192.168.250.13 vault01 192.168.250.21 vault02 192.168.250.22 vault03 192.168.250.23","title":"Nodes"}]}